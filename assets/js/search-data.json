{
  
    
        "post0": {
            "title": "Statistical Rethinking Chapter 6",
            "content": "import numpy as np import pandas as pd import pymc3 as pm import matplotlib.pyplot as plt import seaborn as sns . 6.1 Multicollinearity . X0 -&gt; X1 X0 -&gt; X2 X1 -&gt; y &lt;- X2 Important: Multicollinearity means very strong correlation between two or more predictor variables. The consequence of it is that the posterior distribution will seem to suggest that none of the variables is reliably associated with the outcome, even if all of the variables are in reality strongly associated with the outcome. This frustrating phenomenon arises from the details of how multiple regression works. In fact, there is nothing wrong with multicollinearity. The model will work fine for prediction. You will just be frustrated trying to understand it...Different disciplines have different conventions for dealing with collinear variables. In some fields, it is typical to engage in some kind of data reduction procedure, like principle components or factor analysis, and then to use the components/factors as predictor variables. Note: Above a correlation of 0.9, the standard deviation increases very rapidly, approaching in fact 1 as the correlation approaches 1 . N = 100 np.random.seed(888) height = np.random.normal(10, 2, N) leg_prop = np.random.uniform(0.4, 0.5, N) leg_left = leg_prop * height + np.random.normal(0, 0.02, N) leg_right = leg_prop * height + np.random.normal(0, 0.02, N) d = pd.DataFrame({&#39;height&#39;: height, &#39;leg_left&#39;: leg_left, &#39;leg_right&#39;: leg_right}) d.head() . height leg_left leg_right . 0 9.647598 | 4.747363 | 4.728460 | . 1 10.377753 | 4.377797 | 4.355360 | . 2 11.653494 | 4.785311 | 4.781032 | . 3 9.935105 | 4.674601 | 4.697245 | . 4 8.695001 | 3.814152 | 3.841337 | . X = d[[&#39;leg_left&#39;, &#39;leg_right&#39;]] y = d[&#39;height&#39;] N = 1000 with pm.Model() as model: # Since the outcome and the predictor are both standardized, the intercept should end up very close to zero a = pm.Normal(&#39;a&#39;, 10, 100) b = pm.Normal(&#39;b&#39;, 2, 0.5, shape=X.shape[1]) sigma = pm.Exponential(&#39;sigma&#39;, 1) y = pm.Normal(&#39;y&#39;, mu=a + pm.math.dot(X, b), sigma=sigma, observed=y) mean_q = pm.find_MAP() means = np.concatenate([mean_q[k].reshape(-1) for k in [&#39;a&#39;, &#39;b&#39;, &#39;sigma&#39;]]) cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, b, sigma])) stds = np.sqrt(np.diagonal(cov_q)) post = np.random.multivariate_normal(means, cov_q, size=N) print(&#39;means: &#39;, means.round(3)) print(&#39;stds: &#39;, stds.round(3)) . logp = -106.9, ||grad|| = 0.0044381: 100%|██████████| 43/43 [00:00&lt;00:00, 1102.88it/s] . means: [0.626 1.158 0.922 0.633] stds: [0.316 0.351 0.351 0.045] . fig, axes = plt.subplots(1, 2, figsize=(8, 4)) axes[0].scatter(post[:, 1], post[:, 2]) axes[1].hist(post[:, 1] + post[:, 2]); . 6.2 Post-treatment bias . T -&gt; F -&gt; H1 &lt;- H0 . # initial height, final height, treatment, presence of fungus np.random.seed(888) N = 100 h0 = np.random.normal(10, 2, N) treatment = np.repeat([0, 1], N / 2) fungus = np.random.binomial(1, 0.5 - treatment*0.4, N) h1 = h0 + np.random.normal(5-3*fungus, size=N) d = pd.DataFrame({&#39;h0&#39;: h0, &#39;h1&#39;: h1, &#39;treatment&#39;: treatment, &#39;fungus&#39;: fungus, &#39;h1&#39;: h1}) d.head() . h0 h1 treatment fungus . 0 9.647598 | 11.890146 | 0 | 1 | . 1 10.377753 | 14.950681 | 0 | 0 | . 2 11.653494 | 17.555040 | 0 | 0 | . 3 9.935105 | 12.714568 | 0 | 1 | . 4 8.695001 | 13.496327 | 0 | 0 | . d.groupby(&#39;treatment&#39;).fungus.mean() . treatment 0 0.52 1 0.16 Name: fungus, dtype: float64 . X = d[[&#39;treatment&#39;, &#39;fungus&#39;]] y = d[&#39;h1&#39;] / d[&#39;h0&#39;] N = 1000 with pm.Model() as model: # Since the outcome and the predictor are both standardized, the intercept should end up very close to zero a = pm.Lognormal(&#39;a&#39;, 0, 0.2) b = pm.Normal(&#39;b&#39;, 0, 0.5, shape=X.shape[1]) sigma = pm.Exponential(&#39;sigma&#39;, 1) y = pm.Normal(&#39;y&#39;, mu=a + pm.math.dot(X, b), sigma=sigma, observed=y) mean_q = pm.find_MAP() means = np.concatenate([mean_q[k].reshape(-1) for k in [&#39;a&#39;, &#39;b&#39;, &#39;sigma&#39;]]) cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, b, sigma])) stds = np.sqrt(np.diagonal(cov_q)) post = np.random.multivariate_normal(means, cov_q, size=N) print(&#39;means: &#39;, means.round(3)) print(&#39;stds: &#39;, stds.round(3)) . logp = 22.09, ||grad|| = 434.95: 100%|██████████| 21/21 [00:00&lt;00:00, 1415.40it/s] . means: [ 1.493 0.002 -0.271 0.137] stds: [0.025 0.03 0.031 0.01 ] . The marginal posterior for bt, the effect of treatment, is solidly zero, with a tight interval. The treatment is not associated with growth. The fungus seems to have hurt growth, however. Given that we know the treatment matters, because we built the simulation that way, what happened here? The problem is that fungus is mostly a consequence of treatment. This is to say that fungus is a post-treatment variable. So when we control for fungus, the model is implicitly answering the question:Once we already know whether or not a plant developed fungus, does soil treatment matter? The answer is “no,” because soil treatment has its effects on growth through reducing fungus. But we actually want to know, based on the design of the experiment, is the impact of treatment on growth. To measure this properly, we should omit the post-treatment variable fungus. . Warning: It makes sense to control for pre-treatment differences, like the initial height h0, that might mask the causal influence of treatment. But including post-treatment variables can actually mask the treatment itself...This doesn’t mean you don’t want the model that includes both treatment and fungus. The fact that including fungus zeros the coefficient for treatment suggests that the treatment works for exactly the anticipated reasons. It tells us about mechanism. But a correct inference about the treatment still depends upon omitting the post-treatment variable. . X = d[[&#39;treatment&#39;]] y = d[&#39;h1&#39;] / d[&#39;h0&#39;] N = 1000 with pm.Model() as model: # Since the outcome and the predictor are both standardized, the intercept should end up very close to zero a = pm.Lognormal(&#39;a&#39;, 0, 0.2) b = pm.Normal(&#39;b&#39;, 0, 0.5, shape=X.shape[1]) sigma = pm.Exponential(&#39;sigma&#39;, 1) y = pm.Normal(&#39;y&#39;, mu=a + pm.math.dot(X, b), sigma=sigma, observed=y) mean_q = pm.find_MAP() means = np.concatenate([mean_q[k].reshape(-1) for k in [&#39;a&#39;, &#39;b&#39;, &#39;sigma&#39;]]) cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, b, sigma])) stds = np.sqrt(np.diagonal(cov_q)) post = np.random.multivariate_normal(means, cov_q, size=N) print(&#39;means: &#39;, means.round(3)) print(&#39;stds: &#39;, stds.round(3)) . logp = 26.989, ||grad|| = 4.0574: 100%|██████████| 17/17 [00:00&lt;00:00, 800.27it/s] . means: [1.351 0.1 0.182] stds: [0.026 0.036 0.013] . 6.3 Collider bias . Important: When you condition on a collider, it creates statistical—but not necessarily causal—associations among its causes. . Collider of false sorrow . Happier people are more likely to get married. Another variable that causally influences marriage is age:The more years you are alive, the more likely you are to eventually get married. Putting these three variables together, this is the causal model, with marriage being the collider: H -&gt; M &lt;- A . # Simulation design: # (1) Each year, 20 people are born with uniformly distributed happiness values. # (2) Each year, each person ages one year. Happiness does not change. # (3) At age 18, individuals can become married. The odds of marriage each year are proportional to an individual’s happiness. # (4) Once married, an individual remains married. # (5) After age 65, individuals leave the sample. (They move to Spain.) from scipy.special import expit inv_logit = expit np.random.seed(888) N_years = 1000 N_births = 20 max_age = 65 aom = 18 H = M = A = np.empty(shape=0) for t in range(N_years): A = A + 1 A = np.append(A, np.repeat(1, N_births)) H = np.append(H, np.linspace(-2, 2, N_births)) M = np.append(M, np.repeat(0, N_births)) mask = (A &gt;= aom) &amp; (M == 0) M[mask] = np.random.binomial(1, inv_logit(H[mask]-4), mask.sum()) deaths = A &gt; max_age A = A[~deaths] H = H[~deaths] M = M[~deaths] d = pd.DataFrame({&#39;age&#39;: A, &#39;married&#39;: M, &#39;happiness&#39;: H}) d.describe() . age married happiness . count 1300.000000 | 1300.000000 | 1.300000e+03 | . mean 33.000000 | 0.293846 | 3.416071e-19 | . std 18.768883 | 0.455698 | 1.214421e+00 | . min 1.000000 | 0.000000 | -2.000000e+00 | . 25% 17.000000 | 0.000000 | -1.000000e+00 | . 50% 33.000000 | 0.000000 | -1.110223e-16 | . 75% 49.000000 | 1.000000 | 1.000000e+00 | . max 65.000000 | 1.000000 | 2.000000e+00 | . plt.figure(figsize=(12, 4)) plt.scatter(d.age, d.happiness, c=d.married, cmap=&#39;Paired&#39;) . &lt;matplotlib.collections.PathCollection at 0x1c33b78a10&gt; . d2 = d[d.age &gt; 17].copy() d2[&#39;A&#39;] = (d2.age - 18) / (65 - 18) . with pm.Model() as m: # whether married or not =&gt; index variable a = pm.Normal(&#39;a&#39;, 0, 1, shape=d2[&#39;married&#39;].nunique()) b = pm.Normal(&#39;b&#39;, 0, 2) sigma = pm.Exponential(&#39;sigma&#39;, 1) mu = a[d2[&#39;married&#39;].astype(&#39;category&#39;).cat.codes.values] + b * d2[&#39;A&#39;] happiness = pm.Normal(&#39;happiness&#39;, mu, sigma, observed=d2[&#39;happiness&#39;]) mean_q = pm.find_MAP() means = np.concatenate([mean_q[k].reshape(-1) for k in [&#39;a&#39;, &#39;b&#39;, &#39;sigma&#39;]]) cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, b, sigma])) stds = np.sqrt(np.diagonal(cov_q)) print(&#39;means: &#39;, means.round(3)) print(&#39;stds: &#39;, stds.round(3)) . logp = -1,355.6, ||grad|| = 14.931: 100%|██████████| 16/16 [00:00&lt;00:00, 1828.13it/s] . means: [-0.188 1.337 -0.84 0.987] stds: [0.063 0.087 0.114 0.023] . with pm.Model() as m: a = pm.Normal(&#39;a&#39;, 0, 1) b = pm.Normal(&#39;b&#39;, 0, 2) sigma = pm.Exponential(&#39;sigma&#39;, 1) mu = a + b * d2[&#39;A&#39;] happiness = pm.Normal(&#39;happiness&#39;, mu, sigma, observed=d2[&#39;happiness&#39;]) mean_q = pm.find_MAP() means = np.concatenate([mean_q[k].reshape(-1) for k in [&#39;a&#39;, &#39;b&#39;, &#39;sigma&#39;]]) cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, b, sigma])) stds = np.sqrt(np.diagonal(cov_q)) print(&#39;means: &#39;, means.round(3)) print(&#39;stds: &#39;, stds.round(3)) . logp = -2,005.8, ||grad|| = 1,983.9: 100%|██████████| 10/10 [00:00&lt;00:00, 2762.50it/s] . means: [-0. -0. 1.213] stds: [0.077 0.132 0.028] . Note: The pattern above is exactly what we should expect when we condition on a collider. The collider is marriage status. It a common consequence of age and happiness. As a result, when we condition on it, we induce a spurious association between the two causes. So it looks like that age is negatively associated with happiness. But this is just a statistical association, not a causal association. Once we know whether someone is married or not, then their age does provide information about how happy they are. . The haunted DAG . &lt;= due to unmeasured causes . Suppose for example that we are interested in inferring the direct influence of both parents (P) and grandparents (G) on the educational achievement of children (C)...But suppose there are unmeasured, common influences (U) on parents and their children, such as neighborhoods, that are not shared by grandparents. . G -&gt; P G -&gt;C &lt;- P P &lt;- U -&gt; C . Important: Now P is a common consequence of G and U, so if we condition on P, it will bias inference about G -&gt; C, even if we never get to measure U. . N = 200 b_GP = 1 b_GC = 0 b_PC = 1 b_U = 2 np.random.seed(888) U = 2 * np.random.binomial(1, 0.5, N) - 1 G = np.random.normal(size=N) P = np.random.normal(b_GP*G + b_U*U, size=N) C = np.random.normal(b_PC*P + b_GC*G + b_U*U, size=N) d = pd.DataFrame({&#39;C&#39;: C, &#39;P&#39;: P, &#39;G&#39;: G, &#39;U&#39;: U}) . X = d[[&#39;P&#39;, &#39;G&#39;]] y = d[&#39;C&#39;] N = 1000 with pm.Model() as model: # Since the outcome and the predictor are both standardized, the intercept should end up very close to zero a = pm.Lognormal(&#39;a&#39;, 0, 1) b = pm.Normal(&#39;b&#39;, 0, 1, shape=X.shape[1]) sigma = pm.Exponential(&#39;sigma&#39;, 1) y = pm.Normal(&#39;y&#39;, mu=a + pm.math.dot(X, b), sigma=sigma, observed=y) mean_q = pm.find_MAP() means = np.concatenate([mean_q[k].reshape(-1) for k in [&#39;a&#39;, &#39;b&#39;, &#39;sigma&#39;]]) cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, b, sigma])) stds = np.sqrt(np.diagonal(cov_q)) post = np.random.multivariate_normal(means, cov_q, size=N) print(&#39;means: &#39;, means.round(3)) print(&#39;stds: &#39;, stds.round(3)) . logp = -340.35, ||grad|| = 13.678: 100%|██████████| 21/21 [00:00&lt;00:00, 1543.00it/s] . means: [ 0.233 1.86 -0.814 1.268] stds: [0.09 0.041 0.098 0.063] . The inferred effect of parents looks too big, almost twice as large as it should be. That isn’t surprising. Some of the correlation between P and C is due to U, and the model doesn’t know about U. That’s a simple confound. More surprising is that the model is confident that the direct effect of grandparents is to hurt their grandkids. The regression is not wrong. But a causal interpretation of that association would be...More educated grandparents have more educated grandkids, but this effect arises entirely through parents. Why? Because we assumed it is so. The direct effect of G in the simulation is zero. So consider two different parents with the same education level, say for example at the median 50th centile. One of these parents has a highly educated grandparent. The other has a poorly educated grandparent. The only probable way, in this example, for these parents to have the same education is if they live in different types of neighborhoods. We can’t see these neighborhood effects—we haven’t measured them, recall—but the influence of neighborhood is still transmitted to the children C. So for our mythical two P with the same education, the one with the highly educated G ends up with a less well educated C. The one with the less educated G ends up with the better educated C. G predicts lower C. . lower, upper = d.P.quantile([0.45, 0.6]) mask = (d.P &gt; lower) &amp; (d.P &lt; upper) plt.scatter(d.G[mask], d.C[mask], c=d.U[mask], cmap=&#39;Paired&#39;) plt.scatter(d.G[~mask], d.C[~mask], c=d.U[~mask], alpha=0.25, cmap=&#39;Paired&#39;) . &lt;matplotlib.collections.PathCollection at 0x1c33a9d4d0&gt; . X = d[[&#39;P&#39;, &#39;G&#39;, &#39;U&#39;]] y = d[&#39;C&#39;] N = 1000 with pm.Model() as model: # Since the outcome and the predictor are both standardized, the intercept should end up very close to zero a = pm.Lognormal(&#39;a&#39;, 0, 1) b = pm.Normal(&#39;b&#39;, 0, 1, shape=X.shape[1]) sigma = pm.Exponential(&#39;sigma&#39;, 1) y = pm.Normal(&#39;y&#39;, mu=a + pm.math.dot(X, b), sigma=sigma, observed=y) mean_q = pm.find_MAP() means = np.concatenate([mean_q[k].reshape(-1) for k in [&#39;a&#39;, &#39;b&#39;, &#39;sigma&#39;]]) cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, b, sigma])) stds = np.sqrt(np.diagonal(cov_q)) post = np.random.multivariate_normal(means, cov_q, size=N) print(&#39;means: &#39;, means.round(3)) print(&#39;stds: &#39;, stds.round(3)) . logp = -301, ||grad|| = 1.7393: 100%|██████████| 28/28 [00:00&lt;00:00, 1534.11it/s] . means: [ 0.18 1.154 -0.165 1.709 1.052] stds: [0.075 0.082 0.106 0.179 0.052] . Warning: The grandparents example serves as an example of Simpson’s paradox: Including another predictor (P in this case) can reverse the direction of association between some other predictor (G) and the outcome (C). Usually, Simpson’s paradox is presented in cases where adding the new predictor helps us. But in this case, it misleads us. Simpson’s paradox is a statistical phenomenon. . Practice . 6H1 . from sklearn.preprocessing import StandardScaler d = pd.read_csv( &#39;https://raw.githubusercontent.com/rmcelreath/rethinking/Experimental/data/WaffleDivorce.csv&#39;, sep=&#39;;&#39;) scaler = StandardScaler() d[[&#39;W&#39;, &#39;D&#39;]] = pd.DataFrame(scaler.fit_transform(d[[&#39;WaffleHouses&#39;, &#39;Divorce&#39;]])) d.head() . Location Loc Population MedianAgeMarriage Marriage Marriage SE Divorce Divorce SE WaffleHouses South Slaves1860 Population1860 PropSlaves1860 W D . 0 Alabama | AL | 4.78 | 25.3 | 20.2 | 1.27 | 12.7 | 0.79 | 128 | 1 | 435080 | 964201 | 0.45 | 1.468792 | 1.671000 | . 1 Alaska | AK | 0.71 | 25.2 | 26.0 | 2.93 | 12.5 | 2.05 | 0 | 0 | 0 | 0 | 0.00 | -0.496558 | 1.560044 | . 2 Arizona | AZ | 6.33 | 25.8 | 20.3 | 0.98 | 10.8 | 0.74 | 18 | 0 | 0 | 0 | 0.00 | -0.220181 | 0.616916 | . 3 Arkansas | AR | 2.92 | 24.3 | 26.4 | 1.70 | 13.5 | 1.22 | 41 | 1 | 111115 | 435450 | 0.26 | 0.132968 | 2.114824 | . 4 California | CA | 37.25 | 26.8 | 19.1 | 0.39 | 8.0 | 0.24 | 0 | 0 | 0 | 379994 | 0.00 | -0.496558 | -0.936470 | . with pm.Model() as m: # whether south or not =&gt; index variable a = pm.Normal(&#39;a&#39;, 0, 1, shape=d[&#39;South&#39;].nunique()) b = pm.Normal(&#39;b&#39;, 0, 2) sigma = pm.Exponential(&#39;sigma&#39;, 1) mu = a[d[&#39;South&#39;].astype(&#39;category&#39;).cat.codes.values] + b * d[&#39;W&#39;] D = pm.Normal(&#39;D&#39;, mu, sigma, observed=d[&#39;D&#39;]) mean_q = pm.find_MAP() means = np.concatenate([mean_q[k].reshape(-1) for k in [&#39;a&#39;, &#39;b&#39;, &#39;sigma&#39;]]) cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, b, sigma])) stds = np.sqrt(np.diagonal(cov_q)) print(&#39;means: &#39;, means.round(3)) print(&#39;stds: &#39;, stds.round(3)) . logp = -72.282, ||grad|| = 7.1287e-05: 100%|██████████| 12/12 [00:00&lt;00:00, 920.19it/s] . means: [-0.188 0.467 0.052 0.93 ] stds: [0.17 0.303 0.176 0.092] . with pm.Model() as m: # whether married or not =&gt; index variable a = pm.Normal(&#39;a&#39;, 0, 1) b = pm.Normal(&#39;b&#39;, 0, 2) sigma = pm.Exponential(&#39;sigma&#39;, 1) mu = a + b * d[&#39;W&#39;] D = pm.Normal(&#39;D&#39;, mu, sigma, observed=d[&#39;D&#39;]) mean_q = pm.find_MAP() means = np.concatenate([mean_q[k].reshape(-1) for k in [&#39;a&#39;, &#39;b&#39;, &#39;sigma&#39;]]) cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, b, sigma])) stds = np.sqrt(np.diagonal(cov_q)) print(&#39;means: &#39;, means.round(3)) print(&#39;stds: &#39;, stds.round(3)) . logp = -82.88, ||grad|| = 59.584: 100%|██████████| 10/10 [00:00&lt;00:00, 1970.73it/s] . means: [-0. 0.253 0.958] stds: [0.134 0.135 0.094] .",
            "url": "https://nilichen.github.io/dataING/statistical_rethinking/2020/03/27/chapter6.html",
            "relUrl": "/statistical_rethinking/2020/03/27/chapter6.html",
            "date": " • Mar 27, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Statistical Rethinking Chapter 5",
            "content": "import numpy as np import pandas as pd import pymc3 as pm import matplotlib.pyplot as plt import seaborn as sns . from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = &quot;all&quot; . from sklearn.preprocessing import StandardScaler d = pd.read_csv( &#39;https://raw.githubusercontent.com/rmcelreath/rethinking/Experimental/data/WaffleDivorce.csv&#39;, sep=&#39;;&#39;) scaler = StandardScaler() d[[&#39;A&#39;, &#39;M&#39;, &#39;D&#39;]] = pd.DataFrame(scaler.fit_transform(d[[&#39;MedianAgeMarriage&#39;, &#39;Marriage&#39;, &#39;Divorce&#39;]])) . sns.pairplot(d[[&#39;A&#39;, &#39;M&#39;, &#39;D&#39;]]) . &lt;seaborn.axisgrid.PairGrid at 0x127c8d390&gt; . 5.1 Spurious association . Marriage rate . N = 1000 with pm.Model() as m5_1: # Since the outcome and the predictor are both standardized, the intercept should end up very close to zero a = pm.Normal(&#39;a&#39;, 0, 0.2) bM = pm.Normal(&#39;bM&#39;, 0, 0.5) sigma = pm.Exponential(&#39;sigma&#39;, 1) mu = a + bM * d.M D = pm.Normal(&#39;D&#39;, mu=mu, sigma=sigma, observed=d.D) mean_q = pm.find_MAP() cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, bM, sigma])) . logp = -79.884, ||grad|| = 66.043: 100%|██████████| 10/10 [00:00&lt;00:00, 1705.35it/s] . Prior | . # extract.prior with m5_1: prior = pm.sample_prior_predictive(N) . x_seq = np.arange(-2, 3) a, bM = prior[&#39;a&#39;], prior[&#39;bM&#39;] for i in range(50): y = a[i] + bM[i] * x_seq plt.plot(x_seq, y, c=&#39;k&#39;, alpha=0.2) plt.show(); . Posterior | . x_seq = np.linspace(-3, 3.2, 30) # link post = np.random.multivariate_normal( [mean_q[k] for k in [&#39;a&#39;, &#39;bM&#39;, &#39;sigma&#39;]], cov_q, size=N) mu = np.apply_along_axis( lambda x: post[:, 0] + post[:, 1] * x, axis=1, arr=x_seq[:, np.newaxis]) mu_mean = mu.mean(axis=1) mu_PI = np.quantile(mu, [0.055, 0.945], axis=1) . plt.scatter(d.M, d.D, alpha=0.2) plt.plot(x_seq, mu_mean, c=&#39;k&#39;) plt.fill_between( x_seq, mu_PI[0], mu_PI[1], color=&#39;grey&#39;) plt.xlabel(&#39;Marriage rate&#39;) plt.ylabel(&#39;Divorce rate&#39;) . &lt;matplotlib.collections.PathCollection at 0x12976e210&gt; . [&lt;matplotlib.lines.Line2D at 0x1242f52d0&gt;] . &lt;matplotlib.collections.PolyCollection at 0x12831ef50&gt; . Text(0.5, 0, &#39;Marriage rate&#39;) . Text(0, 0.5, &#39;Divorce rate&#39;) . Median age marriage . # exercise 1 . Multiple regression . A -&gt; M A -&gt; D . with pm.Model() as m5_3: # Since the outcome and the predictor are both standardized, the intercept should end up very close to zero a = pm.Normal(&#39;a&#39;, 0, 0.2) bA = pm.Normal(&#39;bA&#39;, 0, 0.5) bM = pm.Normal(&#39;bM&#39;, 0, 0.5) sigma = pm.Exponential(&#39;sigma&#39;, 1) mu = a + bA * d.A + bM * d.M D = pm.Normal(&#39;D&#39;, mu=mu, sigma=sigma, observed=d.D) with m5_3: mean_q = pm.find_MAP() cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, bA, bM, sigma])) . logp = -61.064, ||grad|| = 0.004354: 100%|██████████| 13/13 [00:00&lt;00:00, 1431.09it/s] . Important: Once we know median age at marriage for a State, there is little or no additional predictive power in also knowing the rate of marriage in that State. . print(f&quot;mean: {np.array([mean_q[k] for k in [&#39;a&#39;, &#39;bA&#39;, &#39;bM&#39;, &#39;sigma&#39;]]).round(3)}&quot;) print(f&#39;std: {np.sqrt(np.diagonal(cov_q)).round(3)}&#39;) . mean: [ 0. -0.614 -0.065 0.793] std: [0.098 0.151 0.151 0.079] . Counterfactual plots =&gt; Partial Dependence Plot . M changes across the range of values in M_seq, while the other predictor is held constant at its mean—which is zero, because A is standardized | . x_seq = np.linspace(-3, 3.2, 30) o_seq = np.zeros(30) # | A=0 | M | X = np.column_stack([o_seq, x_seq]) # link post = np.random.multivariate_normal( [mean_q[k] for k in [&#39;a&#39;, &#39;bA&#39;, &#39;bM&#39;, &#39;sigma&#39;]], cov_q, size=N) mu = np.apply_along_axis( lambda x: post[:, 0] + post[:, 1] * x[0] + post[:, 2] * x[1], axis=1, arr=X) mu_mean = mu.mean(axis=1) mu_PI = np.quantile(mu, [0.055, 0.945], axis=1) . # sim sim_D = np.apply_along_axis( lambda x: np.random.normal(post[:, 0] + post[:, 1] * x[0] + post[:, 2] * x[1], post[:, 3]), axis=1, arr=X) D_PI = np.quantile(sim_D, [0.055, 0.945], axis=1) . plt.plot(x_seq, mu_mean, c=&#39;k&#39;) plt.fill_between( x_seq, mu_PI[0], mu_PI[1], color=&#39;grey&#39;, alpha=0.6) plt.fill_between( x_seq, D_PI[0], D_PI[1], color=&#39;grey&#39;, alpha=0.3); . Warning: In the small world of the model, it is possible to change median age of marriage without also changing the marriage rate. But is this also possible in the large world of reality? Probably not...In that case, while these counterfactual plots always help in understanding the model, they may also mislead by displaying predictions for impossible combinations of predictor values. . M set to its average and A allowed to vary | . # exercise 3 # hint: only need to change one line of code! . Predictor residual plots . the linear relationship between divorce and marriage rates, having statistically “controlled” for median age of marriage. | . with pm.Model() as m5_4: # Since the outcome and the predictor are both standardized, the intercept should end up very close to zero a = pm.Normal(&#39;a&#39;, 0, 0.2) bAM = pm.Normal(&#39;bAM&#39;, 0, 0.5) sigma = pm.Exponential(&#39;sigma&#39;, 1) mu = a + bAM * d.A M = pm.Normal(&#39;M&#39;, mu=mu, sigma=sigma, observed=d.M) mean_q = pm.find_MAP() cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, bAM, sigma])) . logp = -53.827, ||grad|| = 1.2129e-06: 100%|██████████| 12/12 [00:00&lt;00:00, 454.08it/s] . post = np.random.multivariate_normal( [mean_q[k] for k in [&#39;a&#39;, &#39;bAM&#39;, &#39;sigma&#39;]], cov_q, size=N) mu = np.apply_along_axis( lambda x: post[:, 0] + post[:, 1] * x, axis=1, arr=d.A[:, np.newaxis]) mu_mean = mu.mean(axis=1) mu_resid = d.M - mu_mean . fig, axes = plt.subplots(1, 2, figsize=(8, 4)) axes[0].scatter(d.A, d.M) axes[0].plot(d.A, mu_mean, c=&#39;k&#39;) axes[0].set_xlabel(&#39;Median age marriage&#39;) axes[0].set_ylabel(&#39;Marriage rate&#39;) # seaborn trick to fit linear regression sns.regplot(x=mu_resid, y=d.D, ax=axes[1]); axes[1].set_xlabel(&#39;Marriage rate residual&#39;) axes[1].set_ylabel(&#39;Divorce rate&#39;); . The linear relationship between divorce and median age marriage, having statistically “controlled” for marriage rate | . # exercise 2 . Posterior prediction plots. . Note: What is unusual about Idaho and Utah? Both of these States have large proportions of members of the Church of Jesus Christ of Latter-day Saints, known sometimes as Mormons. Members of this church have low rates of divorce, wherever they live. This suggests that having a finer view on the demographic composition of each State, beyond just median age at marriage, would help a lot to refine our understanding. . 5.2 Masked relationship . Important: A second reason to use more than one predictor variable is to measure the direct influences of multiple factors on an outcome, when none of those influences is apparent from bivariate relationships. This kind of problem tends to arise when there are two predictor variables that are correlated with one another. However, one of these is positively correlated with the outcome and the other is negatively correlated with it. . d = pd.read_csv(&#39;https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/milk.csv&#39;, sep=&#39;;&#39;) d[&#39;lmass&#39;] = np.log(d[&#39;mass&#39;]) scaler = StandardScaler() d[[&#39;K&#39;, &#39;N&#39;, &#39;M&#39;]] = pd.DataFrame(scaler.fit_transform(d[[&#39;kcal.per.g&#39;, &#39;neocortex.perc&#39;, &#39;lmass&#39;]])) sns.pairplot(d[[&#39;K&#39;, &#39;N&#39;, &#39;M&#39;]]) . &lt;seaborn.axisgrid.PairGrid at 0x1204d0150&gt; . M -&gt; K &lt;- N M -&gt; N . Large animals tend to live a long time. And in such animals, an investment in learning may be a better investment, because learning can be amortized over a longer lifespan. Both large body size and large neocortex then influence milk composition, but in different directions, for different reasons...Body mass (M) influences neocortex percent (N). Both then influence kilocalories in milk (K)...But with the evidence at hand, we cannot easily see which is right. . Difference when simulating . Spurious association | . n = 100 x_real = np.random.normal(size=n) x_spur = np.random.normal(x_real, size=n) y = np.random.normal(x_real, size=n) . Masked relationship | . M = np.random.normal(size=n) N = np.random.normal(M, size=n) K = np.random.normal(N - M, size=n) . 5.3 Categorical variables . Binary categories . Indicator variable $$h_i sim Normal( mu_i, sigma)$$ $$ mu_i sim alpha + beta_i m_i$$ $$ alpha sim Normal(178, 20)$$ $$ beta_m sim Normal(0, 10)$$ $$ sigma sim Uniform(0, 50)$$ Warning: This can make assigning sensible priors a little harder...Furthermore, this approach necessarily assumes there is more uncertainty about one of the categories - Index variable $$h_i sim Normal( mu_i, sigma)$$ $$ mu_i sim alpha_{SEX[i]}$$ $$ alpha_j sim Normal(178, 20), for j=1..2$$ $$ sigma sim Uniform(0, 50)$$ | . d = pd.read_csv(&#39;https://raw.githubusercontent.com/rmcelreath/rethinking/Experimental/data/Howell1.csv&#39;, sep=&#39;;&#39;) . with pm.Model() as m5_8: a = pm.Normal(&#39;a&#39;, 178, 20, shape=d[&#39;male&#39;].nunique()) sigma = pm.Uniform(&#39;sigma&#39;, 0, 50) mu = a[d[&#39;male&#39;].values] height = pm.Normal(&#39;height&#39;, mu, sigma, observed=d[&#39;height&#39;]) mean_q = pm.find_MAP() cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, sigma])) . logp = -2,724.8, ||grad|| = 0.026744: 100%|██████████| 14/14 [00:00&lt;00:00, 1277.03it/s] . mean_q . {&#39;a&#39;: array([135.55461973, 143.16814912]), &#39;sigma_interval__&#39;: array(18.28842903), &#39;sigma&#39;: array(49.99999943)} . cov_q . array([[ 8.46804887, -0.05219947, -1.57797704], [ -0.05219947, 9.44895653, -1.44254616], [ -1.57797704, -1.44254616, -43.60780918]]) . Many categories . d = pd.read_csv(&#39;https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/milk.csv&#39;, sep=&#39;;&#39;) d[&#39;K&#39;] = scaler.fit_transform(d[[&#39;kcal.per.g&#39;]]) . with pm.Model() as m5_9: a = pm.Normal(&#39;a&#39;, 0, 0.5, shape=d[&#39;clade&#39;].nunique()) sigma = pm.Exponential(&#39;sigma&#39;, 1) mu = a[d[&#39;clade&#39;].astype(&#39;category&#39;).cat.codes.values] K = pm.Normal(&#39;K&#39;, mu, sigma, observed=d[&#39;K&#39;]) mean_q = pm.find_MAP() cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, sigma])) . logp = -36.531, ||grad|| = 0.0019242: 100%|██████████| 13/13 [00:00&lt;00:00, 2109.32it/s] . mean_q . {&#39;a&#39;: array([-0.48941463, 0.37008325, 0.68048305, -0.58956155]), &#39;sigma_log__&#39;: array(-0.31019941), &#39;sigma&#39;: array(0.73330072)} . np.sqrt(np.diagonal(cov_q)) . array([0.22105719, 0.2204332 , 0.26134171, 0.27832982, 0.0985117 ]) . Pracitice . Hard . foxes = pd.read_csv(&#39;https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/foxes.csv&#39;, sep=&#39;;&#39;) foxes[[&#39;F&#39;, &#39;G&#39;, &#39;A&#39;, &#39;W&#39;]] = pd.DataFrame(scaler.fit_transform(foxes[[&#39;avgfood&#39;, &#39;groupsize&#39;, &#39;area&#39;, &#39;weight&#39;]])) . sns.pairplot(foxes[[&#39;F&#39;, &#39;G&#39;, &#39;A&#39;, &#39;W&#39;]]) . &lt;seaborn.axisgrid.PairGrid at 0x1256ea250&gt; . def get_post(X, y, N=1000): with pm.Model() as model: # Since the outcome and the predictor are both standardized, the intercept should end up very close to zero a = pm.Normal(&#39;a&#39;, 0, 0.2) b = pm.Normal(&#39;b&#39;, 0, 0.5, shape=X.shape[1]) sigma = pm.Exponential(&#39;sigma&#39;, 1) y = pm.Normal(&#39;y&#39;, mu=a + pm.math.dot(X, b), sigma=sigma, observed=y) mean_q = pm.find_MAP() means = np.concatenate([mean_q[k].reshape(-1) for k in [&#39;a&#39;, &#39;b&#39;, &#39;sigma&#39;]]) cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, b, sigma])) stds = np.sqrt(np.diagonal(cov_q)) post = np.random.multivariate_normal(means, cov_q, size=N) print(&#39;means: &#39;, means.round(3)) print(&#39;stds: &#39;, stds.round(3)) return post . def plot_intervals(post, X): X = np.concatenate((np.ones(30)[:, np.newaxis], X), axis=1) # link mu = X.dot(post[:, :-1].T) mu_mean = mu.mean(axis=1) mu_PI = np.quantile(mu, [0.055, 0.945], axis=1) # sim sim_D = np.random.normal(mu, post[:, -1]) D_PI = np.quantile(sim_D, [0.055, 0.945], axis=1) plt.plot(x_seq, mu_mean, c=&#39;k&#39;) plt.fill_between( x_seq, mu_PI[0], mu_PI[1], color=&#39;grey&#39;, alpha=0.6) plt.fill_between( x_seq, D_PI[0], D_PI[1], color=&#39;grey&#39;, alpha=0.3); . 5H1 (1) | . post = get_post(foxes[[&#39;A&#39;]], foxes[&#39;W&#39;]) . logp = -185.03, ||grad|| = 124.83: 100%|██████████| 9/9 [00:00&lt;00:00, 649.03it/s] . means: [0. 0.019 0.996] stds: [0.084 0.091 0.065] . x_seq = np.linspace(-3, 3.2, 30) plot_intervals(post, x_seq[:, np.newaxis]) . 5H1 (2) | . post = get_post(foxes[[&#39;G&#39;]], foxes[&#39;W&#39;]) x_seq = np.linspace(-3, 3.2, 30) . logp = -185.03, ||grad|| = 130.66: 100%|██████████| 9/9 [00:00&lt;00:00, 1337.28it/s] . means: [ 0. -0.156 0.983] stds: [0.083 0.09 0.064] . plot_intervals(post, x_seq[:, np.newaxis]) . 5H2 | . post = get_post(foxes[[&#39;A&#39;, &#39;G&#39;]], foxes[&#39;W&#39;]) x_seq = np.linspace(-3, 3.2, 30) . logp = -185.26, ||grad|| = 130.75: 100%|██████████| 11/11 [00:00&lt;00:00, 1709.93it/s] . means: [ 0. 0.406 -0.482 0.946] stds: [0.08 0.145 0.145 0.062] . # holding groupsize constant at 0 plot_intervals(post, np.column_stack([np.linspace(-3, 3.2, 30), np.zeros(30)])) . # holding area constant at 0 plot_intervals(post, np.column_stack([np.zeros(30), np.linspace(-3, 3.2, 30)])) . 5H3 | . post = get_post(foxes[[&#39;F&#39;, &#39;G&#39;]], foxes[&#39;W&#39;]) x_seq = np.linspace(-3, 3.2, 30) . logp = -160.46, ||grad|| = 5.1972e-06: 100%|██████████| 12/12 [00:00&lt;00:00, 1871.48it/s] . means: [ 0. 0.477 -0.574 0.946] stds: [0.08 0.179 0.179 0.062] . # holding groupsize constant at 0 plot_intervals(post, np.column_stack([np.linspace(-3, 3.2, 30), np.zeros(30)])) . # holding avgfood constant at 0 plot_intervals(post, np.column_stack([np.zeros(30), np.linspace(-3, 3.2, 30)])) . post = get_post(foxes[[&#39;A&#39;, &#39;F&#39;, &#39;G&#39;]], foxes[&#39;W&#39;]) . logp = -159.37, ||grad|| = 1.2402: 100%|██████████| 16/16 [00:00&lt;00:00, 1092.30it/s] . means: [ 0. 0.278 0.297 -0.64 0.935] stds: [0.08 0.17 0.21 0.182 0.061] .",
            "url": "https://nilichen.github.io/dataING/statistical_rethinking/2020/03/05/chapter5.html",
            "relUrl": "/statistical_rethinking/2020/03/05/chapter5.html",
            "date": " • Mar 5, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Statistical Rethinking Chapter 4",
            "content": "import numpy as np import scipy.stats as stats import pandas as pd import matplotlib.pyplot as plt import pymc3 as pm . df = pd.read_csv(&#39;https://raw.githubusercontent.com/rmcelreath/rethinking/Experimental/data/Howell1.csv&#39;, sep=&#39;;&#39;) . df2 = df[df.age &gt;= 18] . 4.3 Gaussian model of height . Prior predictive simulation . effect of $ mu$ prior | . sample_mu = np.random.normal(178, 20, 10000) sample_sigma = np.random.uniform(0, 50, 10000) prior_h = np.random.normal(sample_mu, sample_sigma) fig, axes = plt.subplots(2, sharex=True) axes[0].hist(prior_h); sample_mu = np.random.normal(178, 100, 10000) prior_h = np.random.normal(sample_mu, sample_sigma) axes[1].hist(prior_h); . Grid Approximation . mu_list = np.linspace(150, 160, 100) sigma_list = np.linspace(7, 9, 100) post_mu, post_sigma = np.array(np.meshgrid(mu_list, sigma_list))#.reshape(2, 10000).T # post.shape . # log likelihood post_LL = np.vectorize( lambda mu, sigma: stats.norm(mu, sigma).logpdf(df2.height).sum() )(post_mu, post_sigma) post_LL.shape . (100, 100) . # unnormalized posterior post_prod = (post_LL + stats.norm(178, 20).logpdf(post_mu) + stats.uniform(0, 50).logpdf(post_sigma)) post_prob = np.exp(post_prod - np.max(post_prod)) # normalized post_prob = post_prob / post_prob.sum() post_prob.shape . (100, 100) . fig, axes = plt.subplots(1, 2, figsize=(10, 4)) axes[0].pcolormesh(post_mu, post_sigma, post_prob, cmap=&#39;Greys&#39;) axes[1].contour(post_mu, post_sigma, post_prob, cmap=&#39;Greys&#39;) # plt.colorbar() . &lt;matplotlib.contour.QuadContourSet at 0x1196f7490&gt; . Sample from the posterior . compare with prior distribution $$ mu sim Normal(178, 20)$$ $$ sigma sim Uniform(0, 50)$$ | . sample_mu = np.random.choice(post_mu.reshape(10000,), size=10000, p=post_prob.reshape(10000,), replace=True) sample_sigma = np.random.choice(post_sigma.reshape(10000,), size=10000, p=post_prob.reshape(10000,), replace=True) . fig, axes = plt.subplots(1, 3, figsize=(12, 4)) axes[0].scatter(sample_mu, sample_sigma, alpha=0.01) # axes[0].xlim(150, 160) axes[1].hist(sample_mu) # axes[1].hist(np.random.normal(178, 20, 10000), alpha=0.1) axes[2].hist(sample_sigma); . sample_mu.shape . (10000,) . Sample size . df3 = df2.sample(20) mu_list = np.linspace(150, 170, 100) sigma_list = np.linspace(4, 20, 100) post_mu, post_sigma = np.array(np.meshgrid(mu_list, sigma_list))#.reshape(2, 10000).T . # log likelihood post_LL = np.vectorize( lambda mu, sigma: stats.norm(mu, sigma).logpdf(df3.height).sum() )(post_mu, post_sigma) post_LL.shape . (100, 100) . # unnormalized posterior post_prod = (post_LL + stats.norm(178, 20).logpdf(post_mu) + stats.uniform(0, 50).logpdf(post_sigma)) post_prob = np.exp(post_prod - np.max(post_prod)) # normalized post_prob = post_prob / post_prob.sum() post_prob.shape . (100, 100) . sample_mu = np.random.choice(post_mu.reshape(10000,), size=10000, p=post_prob.reshape(10000,), replace=True) sample_sigma = np.random.choice(post_sigma.reshape(10000,), size=10000, p=post_prob.reshape(10000,), replace=True) . In principle, the posterior is not always so Gaussian in shape...you do need to be careful of abusing the quadratic approximation...But a useful way to conceive of the problem is that variances must be positive. As a result, there must be more uncertainty about how big the variance (or standard deviation) is than about how small it is. . fig, axes = plt.subplots(1, 3, figsize=(12, 4)) axes[0].scatter(sample_mu, sample_sigma, alpha=0.02) # axes[0].xlim(150, 160) axes[1].hist(sample_mu) # axes[1].hist(np.random.normal(178, 20, 10000), alpha=0.1) axes[2].hist(sample_sigma); . Quadratic approximation . with pm.Model() as normal_approximation: mu = pm.Normal(&#39;mu&#39;, 178, 20) sigma = pm.Uniform(&#39;sigma&#39;, 0, 50) height = pm.Normal(&#39;h&#39;, mu=mu, sigma=sigma, observed=df2.height) # https://docs.pymc.io/notebooks/getting_started.html#Maximum-a-posteriori-methods mean_q = pm.find_MAP() # https://stats.stackexchange.com/questions/68080/basic-question-about-fisher-information-matrix-and-relationship-to-hessian-and-s cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[mu, sigma])) . logp = -1,235.2, ||grad|| = 11.697: 100%|██████████| 19/19 [00:00&lt;00:00, 1548.98it/s] . mean_q . {&#39;mu&#39;: array(154.60702358), &#39;sigma_interval__&#39;: array(-1.69876478), &#39;sigma&#39;: array(7.73133303)} . cov_q . array([[0.16973961, 0.00021803], [0.00021803, 0.08490583]]) . post = np.random.multivariate_normal([mean_q[&#39;mu&#39;], mean_q[&#39;sigma&#39;]], cov_q, size=1000) . fig, axes = plt.subplots(1, 2, figsize=(8, 4)) axes[0].hist(post[:, 0]) axes[1].hist(post[:, 1]); . 4.4 Linear Prediction . Prior . np.random.seed(888) N = 100 a = np.random.normal(178, 20, N) b = np.random.normal(0, 10, N) lb = np.random.lognormal(0, 1, N) fig, axes = plt.subplots(1, 2, figsize=(8, 4), sharey=True) axes[0].hist(b) axes[1].hist(lb); . x = np.array([df2.weight.min(), df2.weight.max()]) xbar = df2.weight.mean() fig, axes = plt.subplots(1, 2, figsize=(8, 4), sharey=True) for i in range(N): y = a[i] + b[i] * (x - xbar) ly = a[i] + lb[i] * (x - xbar) axes[0].plot(x, y, c=&#39;k&#39;, alpha=0.2) axes[1].plot(x, ly, c=&#39;k&#39;, alpha=0.2) . Posterior Distribution . def fit_linear(x, y): with pm.Model() as normal_approximation: a = pm.Normal(&#39;a&#39;, 178, 20) b = pm.Lognormal(&#39;b&#39;, 0, 1) sigma = pm.Uniform(&#39;sigma&#39;, 0, 50) mu = a + b * (x - x.mean()) height = pm.Normal(&#39;h&#39;, mu=mu, sigma=sigma, observed=y) mean_q = pm.find_MAP() cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, b, sigma])) return mean_q, cov_q . mean_q, cov_q = fit_linear(df2.weight, df2.height) . logp = -1,080.4, ||grad|| = 5.147: 100%|██████████| 39/39 [00:00&lt;00:00, 2308.66it/s] . mean_q . {&#39;a&#39;: array(154.60136748), &#39;b_log__&#39;: array(-0.10172172), &#39;sigma_interval__&#39;: array(-2.18135226), &#39;b&#39;: array(0.90328088), &#39;sigma&#39;: array(5.07188029)} . cov_q.round(3) . array([[ 0.073, -0. , 0. ], [-0. , 0.002, -0. ], [ 0. , -0. , 0.037]]) . post = np.random.multivariate_normal( [mean_q[&#39;a&#39;], mean_q[&#39;b&#39;], mean_q[&#39;sigma&#39;]], cov_q, size=1000) a_map = post[:, 0].mean() b_map = post[:, 1].mean() plt.scatter(df2.weight, df2.height) plt.plot(x, a_map + b_map * (x - df2.weight.mean()), c=&#39;k&#39;, linewidth=2) . [&lt;matplotlib.lines.Line2D at 0x11d0d54d0&gt;] . Uncertainty . def plot_lines(df, ax, n=20): mean_q, cov_q = fit_linear(df.weight, df.height) post = np.random.multivariate_normal( [mean_q[&#39;a&#39;], mean_q[&#39;b&#39;], mean_q[&#39;sigma&#39;]], cov_q, size=n) ax.scatter(df.weight, df.height) for i in range(n): y = post[i, 0] + post[i, 1] * (x - xbar) ax.plot(x, y, c=&#39;k&#39;, alpha=0.2) . fig, axes = plt.subplots(1, 2, figsize=(8, 4), sharey=True) plot_lines(df2[:10], axes[0]) plot_lines(df2, axes[1]) . logp = -38.38, ||grad|| = 3.086e-05: 100%|██████████| 32/32 [00:00&lt;00:00, 2187.88it/s] logp = -1,080.4, ||grad|| = 5.147: 100%|██████████| 39/39 [00:00&lt;00:00, 2800.13it/s] . Regression Intervals . weight_seq = np.arange(25, 71) # link mu = np.apply_along_axis( lambda weight: post[:, 0] + post[:, 1] * (weight - xbar), axis=1, arr=weight_seq[:, np.newaxis]) . mu_mean = mu.mean(axis=1) mu_PI = np.quantile(mu, [0.055, 0.945], axis=1) . # mu_PI . fig, axes = plt.subplots(1, 2, figsize=(8, 4)) for i in range(100): axes[0].scatter(weight_seq, mu[:, i], c=&#39;b&#39;, alpha=0.02, s=5) axes[1].scatter(df2.weight, df2.height, alpha=0.2) axes[1].plot(weight_seq, mu_mean, c=&#39;k&#39;) axes[1].fill_between( weight_seq, mu_PI[0], mu_PI[1], color=&#39;grey&#39;, alpha=0.6) . &lt;matplotlib.collections.PolyCollection at 0x11f0b92d0&gt; . Prediction Intervals . Rethinking:Two kinds of uncertainty . # sim sim_height = np.apply_along_axis( lambda weight: np.random.normal(post[:, 0] + post[:, 1] * (weight - xbar), post[:, 2]), axis=1, arr=weight_seq[:, np.newaxis]) # sim_height = np.random.normal(mu, post[:, 2]) . height_PI = np.quantile(sim_height, [0.055, 0.945], axis=1) . plt.scatter(df2.weight, df2.height, alpha=0.2) plt.plot(weight_seq, mu_mean, c=&#39;k&#39;) plt.fill_between( weight_seq, mu_PI[0], mu_PI[1], color=&#39;grey&#39;, alpha=0.6) plt.fill_between( weight_seq, height_PI[0], height_PI[1], color=&#39;grey&#39;, alpha=0.3) . &lt;matplotlib.collections.PolyCollection at 0x11fb71950&gt; . Practice . plt.scatter(df.weight, df.height, c=df.age &gt;= 18, cmap=&#39;tab10&#39;) . &lt;matplotlib.collections.PathCollection at 0x11e65afd0&gt; . weight_seq = np.array([46.95, 43.72, 64.78, 32.59, 54.63]) sim_height = np.apply_along_axis( lambda weight: np.random.normal(post[:, 0] + post[:, 1] * (weight - xbar), post[:, 2]), axis=1, arr=weight_seq[:, np.newaxis]) . height_PI = np.quantile(sim_height, [0.055, 0.945], axis=1) . height_PI . array([[148.39192365, 145.50397526, 164.59922844, 134.62916833, 155.53171926], [164.47614296, 160.96243128, 180.80917257, 150.78640977, 171.70537767]]) .",
            "url": "https://nilichen.github.io/dataING/statistical_rethinking/2020/03/03/chapter4.html",
            "relUrl": "/statistical_rethinking/2020/03/03/chapter4.html",
            "date": " • Mar 3, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Statistical Rethinking Chapter 3 Practice",
            "content": "import random import numpy as np import scipy.stats as stats import matplotlib.pyplot as plt import pymc3 . from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = &quot;all&quot; . def binormial_posterior(p_grid, prior, n, N): assert(len(p_grid) == len(prior)) likelihood = stats.binom.pmf(n, N, p_grid) posterior = likelihood * prior posterior = posterior / sum(posterior) return posterior def sample_posterior(p_grid, posterior, sample_size): assert(len(p_grid) == len(posterior)) samples = np.random.choice(p_grid, size=sample_size, p=posterior, replace=True) return samples . Easy . random.seed(100) p_grid = np.linspace(0, 1, 1000) prior = np.ones(1000) posterior = binormial_posterior(p_grid, prior, 6, 9) samples = sample_posterior(p_grid, posterior, 10000) (samples &lt; 0.2).mean() (samples &gt; 0.8).mean() ((samples &gt; 0.2) &amp; (samples &lt; 0.8)).mean() np.quantile(samples, 0.2) np.quantile(samples, 0.8) pymc3.stats.hpd(samples, 0.66) np.quantile(samples, [0.17, 0.83]) . 0.0008 . 0.1179 . 0.8813 . 0.5125125125125125 . 0.7587587587587588 . array([0.52352352, 0.7977978 ]) . array([0.49349349, 0.77277277]) . Medium . random.seed(100) prior = np.ones(1000) posterior = binormial_posterior(p_grid, prior, 8, 15) samples = sample_posterior(p_grid, posterior, 10000) pymc3.stats.hpd(samples, 0.9) (np.random.binomial(15, samples) == 8).mean() (np.random.binomial(9, samples) == 6).mean() . array([0.33233233, 0.72172172]) . 0.1475 . 0.1815 . prior = np.hstack([np.zeros(500), 2*np.ones(500)]) random.seed(100) posterior = binormial_posterior(p_grid, prior, 8, 15) samples = sample_posterior(p_grid, posterior, 10000) pymc3.stats.hpd(samples, 0.9) print((np.random.binomial(15, samples) == 8).mean(), stats.binom.pmf(8, 15, 0.7)) print((np.random.binomial(9, samples) == 6).mean(), stats.binom.pmf(6, 9, 0.7)) . array([0.5005005 , 0.71171171]) . 0.1544 0.08113003332934526 0.2351 0.2668279319999999 . Hard . birth1 = np.array([1,0,0,0,1,1,0,1,0,1,0,0,1,1,0,1,1,0,0,0,1,0,0,0,1,0, 0,0,0,1,1,1,0,1,0,1,1,1,0,1,0,1,1,0,1,0,0,1,1,0,1,0,0,0,0,0,0,0, 1,1,0,1,0,0,1,0,0,0,1,0,0,1,1,1,1,0,1,0,1,1,1,1,1,0,0,1,0,1,1,0, 1,0,1,1,1,0,1,1,1,1]) birth2 = np.array([0,1,0,1,0,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,0,0,1,1,1,0, 1,1,1,0,1,1,1,0,1,0,0,1,1,1,1,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1, 1,1,1,0,1,1,0,1,1,0,1,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,1,0,0,1,1, 0,0,0,1,1,1,0,0,0,0]) . n = birth1.sum() + birth2.sum() n . 111 . prior = np.ones(1000) random.seed(100) posterior = binormial_posterior(p_grid, prior, n, 200) p_grid[posterior==posterior.max()] . array([0.55455455]) . samples = sample_posterior(p_grid, posterior, 10000) [pymc3.stats.hpd(samples, iv) for iv in [0.5, 0.89, 0.97]] . [array([0.53153153, 0.57757758]), array([0.5015015 , 0.61161161]), array([0.47747748, 0.62662663])] . samples_n = np.random.binomial(200, samples) plt.hist(samples_n) plt.axvline(x=n, c=&#39;r&#39;); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; samples_n = np.random.binomial(100, samples) plt.hist(samples_n) plt.axvline(x=birth1.sum(), c=&#39;r&#39;); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; samples_n = np.random.binomial((birth1==0).sum(), samples) plt.hist(samples_n) plt.axvline(x=birth2[birth1==0].sum(), c=&#39;r&#39;); # NOT INDEPENDENT . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt;",
            "url": "https://nilichen.github.io/dataING/statistical_rethinking/2020/02/20/chapter3.html",
            "relUrl": "/statistical_rethinking/2020/02/20/chapter3.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Example Markdown Post",
            "content": "Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . You can include alert boxes …and… . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://nilichen.github.io/dataING/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://nilichen.github.io/dataING/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  
  

  

  
  

  

  
      ,"page8": {
          "title": "",
          "content": "Warning . Do not manually save images into this folder. This is used by GitHub Actions to automatically copy images. Any images you save into this folder could be deleted at build time. .",
          "url": "https://nilichen.github.io/dataING/images/copied_from_nb/",
          "relUrl": "/images/copied_from_nb/",
          "date": ""
      }
      
  

  
  

}