{
  
    
        "post0": {
            "title": "Statistical Rethinking Chapter 4",
            "content": "import numpy as np import scipy.stats as stats import pandas as pd import matplotlib.pyplot as plt import pymc3 as pm . df = pd.read_csv(&#39;../datasets/Howell1.csv&#39;, sep=&#39;;&#39;) . df2 = df[df.age &gt;= 18] . 4.3 Gaussian model of height . Prior predictive simulation . effect of $ mu$ prior | . sample_mu = np.random.normal(178, 20, 10000) sample_sigma = np.random.uniform(0, 50, 10000) prior_h = np.random.normal(sample_mu, sample_sigma) fig, axes = plt.subplots(2, sharex=True) axes[0].hist(prior_h); sample_mu = np.random.normal(178, 100, 10000) prior_h = np.random.normal(sample_mu, sample_sigma) axes[1].hist(prior_h); . Grid Approximation . mu_list = np.linspace(150, 160, 100) sigma_list = np.linspace(7, 9, 100) post_mu, post_sigma = np.array(np.meshgrid(mu_list, sigma_list))#.reshape(2, 10000).T # post.shape . # log likelihood post_LL = np.vectorize( lambda mu, sigma: stats.norm(mu, sigma).logpdf(df2.height).sum() )(post_mu, post_sigma) post_LL.shape . (100, 100) . # unnormalized posterior post_prod = (post_LL + stats.norm(178, 20).logpdf(post_mu) + stats.uniform(0, 50).logpdf(post_sigma)) post_prob = np.exp(post_prod - np.max(post_prod)) # normalized post_prob = post_prob / post_prob.sum() post_prob.shape . (100, 100) . fig, axes = plt.subplots(1, 2, figsize=(10, 4)) axes[0].pcolormesh(post_mu, post_sigma, post_prob, cmap=&#39;Greys&#39;) axes[1].contour(post_mu, post_sigma, post_prob, cmap=&#39;Greys&#39;) # plt.colorbar() . &lt;matplotlib.contour.QuadContourSet at 0x1196f7490&gt; . Sample from the posterior . compare with prior distribution $$ mu sim Normal(178, 20)$$ $$ sigma sim Uniform(0, 50)$$ | . sample_mu = np.random.choice(post_mu.reshape(10000,), size=10000, p=post_prob.reshape(10000,), replace=True) sample_sigma = np.random.choice(post_sigma.reshape(10000,), size=10000, p=post_prob.reshape(10000,), replace=True) . fig, axes = plt.subplots(1, 3, figsize=(12, 4)) axes[0].scatter(sample_mu, sample_sigma, alpha=0.01) # axes[0].xlim(150, 160) axes[1].hist(sample_mu) # axes[1].hist(np.random.normal(178, 20, 10000), alpha=0.1) axes[2].hist(sample_sigma); . sample_mu.shape . (10000,) . Sample size . df3 = df2.sample(20) mu_list = np.linspace(150, 170, 100) sigma_list = np.linspace(4, 20, 100) post_mu, post_sigma = np.array(np.meshgrid(mu_list, sigma_list))#.reshape(2, 10000).T . # log likelihood post_LL = np.vectorize( lambda mu, sigma: stats.norm(mu, sigma).logpdf(df3.height).sum() )(post_mu, post_sigma) post_LL.shape . (100, 100) . # unnormalized posterior post_prod = (post_LL + stats.norm(178, 20).logpdf(post_mu) + stats.uniform(0, 50).logpdf(post_sigma)) post_prob = np.exp(post_prod - np.max(post_prod)) # normalized post_prob = post_prob / post_prob.sum() post_prob.shape . (100, 100) . sample_mu = np.random.choice(post_mu.reshape(10000,), size=10000, p=post_prob.reshape(10000,), replace=True) sample_sigma = np.random.choice(post_sigma.reshape(10000,), size=10000, p=post_prob.reshape(10000,), replace=True) . In principle, the posterior is not always so Gaussian in shape...you do need to be careful of abusing the quadratic approximation...But a useful way to conceive of the problem is that variances must be positive. As a result, there must be more uncertainty about how big the variance (or standard deviation) is than about how small it is. . fig, axes = plt.subplots(1, 3, figsize=(12, 4)) axes[0].scatter(sample_mu, sample_sigma, alpha=0.02) # axes[0].xlim(150, 160) axes[1].hist(sample_mu) # axes[1].hist(np.random.normal(178, 20, 10000), alpha=0.1) axes[2].hist(sample_sigma); . Quadratic approximation . with pm.Model() as normal_approximation: mu = pm.Normal(&#39;mu&#39;, 178, 20) sigma = pm.Uniform(&#39;sigma&#39;, 0, 50) height = pm.Normal(&#39;h&#39;, mu=mu, sigma=sigma, observed=df2.height) # https://docs.pymc.io/notebooks/getting_started.html#Maximum-a-posteriori-methods mean_q = pm.find_MAP() # https://stats.stackexchange.com/questions/68080/basic-question-about-fisher-information-matrix-and-relationship-to-hessian-and-s cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[mu, sigma])) . logp = -1,235.2, ||grad|| = 11.697: 100%|██████████| 19/19 [00:00&lt;00:00, 1548.98it/s] . mean_q . {&#39;mu&#39;: array(154.60702358), &#39;sigma_interval__&#39;: array(-1.69876478), &#39;sigma&#39;: array(7.73133303)} . cov_q . array([[0.16973961, 0.00021803], [0.00021803, 0.08490583]]) . post = np.random.multivariate_normal([mean_q[&#39;mu&#39;], mean_q[&#39;sigma&#39;]], cov_q, size=1000) . fig, axes = plt.subplots(1, 2, figsize=(8, 4)) axes[0].hist(post[:, 0]) axes[1].hist(post[:, 1]); . 4.4 Linear Prediction . Prior . np.random.seed(888) N = 100 a = np.random.normal(178, 20, N) b = np.random.normal(0, 10, N) lb = np.random.lognormal(0, 1, N) fig, axes = plt.subplots(1, 2, figsize=(8, 4), sharey=True) axes[0].hist(b) axes[1].hist(lb); . x = np.array([df2.weight.min(), df2.weight.max()]) xbar = df2.weight.mean() fig, axes = plt.subplots(1, 2, figsize=(8, 4), sharey=True) for i in range(N): y = a[i] + b[i] * (x - xbar) ly = a[i] + lb[i] * (x - xbar) axes[0].plot(x, y, c=&#39;k&#39;, alpha=0.2) axes[1].plot(x, ly, c=&#39;k&#39;, alpha=0.2) . Posterior Distribution . def fit_linear(x, y): with pm.Model() as normal_approximation: a = pm.Normal(&#39;a&#39;, 178, 20) b = pm.Lognormal(&#39;b&#39;, 0, 1) sigma = pm.Uniform(&#39;sigma&#39;, 0, 50) mu = a + b * (x - x.mean()) height = pm.Normal(&#39;h&#39;, mu=mu, sigma=sigma, observed=y) mean_q = pm.find_MAP() cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, b, sigma])) return mean_q, cov_q . mean_q, cov_q = fit_linear(df2.weight, df2.height) . logp = -1,080.4, ||grad|| = 5.147: 100%|██████████| 39/39 [00:00&lt;00:00, 2308.66it/s] . mean_q . {&#39;a&#39;: array(154.60136748), &#39;b_log__&#39;: array(-0.10172172), &#39;sigma_interval__&#39;: array(-2.18135226), &#39;b&#39;: array(0.90328088), &#39;sigma&#39;: array(5.07188029)} . cov_q.round(3) . array([[ 0.073, -0. , 0. ], [-0. , 0.002, -0. ], [ 0. , -0. , 0.037]]) . post = np.random.multivariate_normal( [mean_q[&#39;a&#39;], mean_q[&#39;b&#39;], mean_q[&#39;sigma&#39;]], cov_q, size=1000) a_map = post[:, 0].mean() b_map = post[:, 1].mean() plt.scatter(df2.weight, df2.height) plt.plot(x, a_map + b_map * (x - df2.weight.mean()), c=&#39;k&#39;, linewidth=2) . [&lt;matplotlib.lines.Line2D at 0x11d0d54d0&gt;] . Uncertainty . def plot_lines(df, ax, n=20): mean_q, cov_q = fit_linear(df.weight, df.height) post = np.random.multivariate_normal( [mean_q[&#39;a&#39;], mean_q[&#39;b&#39;], mean_q[&#39;sigma&#39;]], cov_q, size=n) ax.scatter(df.weight, df.height) for i in range(n): y = post[i, 0] + post[i, 1] * (x - xbar) ax.plot(x, y, c=&#39;k&#39;, alpha=0.2) . fig, axes = plt.subplots(1, 2, figsize=(8, 4), sharey=True) plot_lines(df2[:10], axes[0]) plot_lines(df2, axes[1]) . logp = -38.38, ||grad|| = 3.086e-05: 100%|██████████| 32/32 [00:00&lt;00:00, 2187.88it/s] logp = -1,080.4, ||grad|| = 5.147: 100%|██████████| 39/39 [00:00&lt;00:00, 2800.13it/s] . Regression Intervals . weight_seq = np.arange(25, 71) # link mu = np.apply_along_axis( lambda weight: post[:, 0] + post[:, 1] * (weight - xbar), axis=1, arr=weight_seq[:, np.newaxis]) . mu_mean = mu.mean(axis=1) mu_PI = np.quantile(mu, [0.055, 0.945], axis=1) . # mu_PI . fig, axes = plt.subplots(1, 2, figsize=(8, 4)) for i in range(100): axes[0].scatter(weight_seq, mu[:, i], c=&#39;b&#39;, alpha=0.02, s=5) axes[1].scatter(df2.weight, df2.height, alpha=0.2) axes[1].plot(weight_seq, mu_mean, c=&#39;k&#39;) axes[1].fill_between( weight_seq, mu_PI[0], mu_PI[1], color=&#39;grey&#39;, alpha=0.6) . &lt;matplotlib.collections.PolyCollection at 0x11f0b92d0&gt; . Prediction Intervals . Rethinking: Two kinds of uncertainty . # sim sim_height = np.apply_along_axis( lambda weight: np.random.normal(post[:, 0] + post[:, 1] * (weight - xbar), post[:, 2]), axis=1, arr=weight_seq[:, np.newaxis]) # sim_height = np.random.normal(mu, post[:, 2]) . height_PI = np.quantile(sim_height, [0.055, 0.945], axis=1) . plt.scatter(df2.weight, df2.height, alpha=0.2) plt.plot(weight_seq, mu_mean, c=&#39;k&#39;) plt.fill_between( weight_seq, mu_PI[0], mu_PI[1], color=&#39;grey&#39;, alpha=0.6) plt.fill_between( weight_seq, height_PI[0], height_PI[1], color=&#39;grey&#39;, alpha=0.3) . &lt;matplotlib.collections.PolyCollection at 0x11fb71950&gt; . Practice . plt.scatter(df.weight, df.height, c=df.age &gt;= 18, cmap=&#39;tab10&#39;) . &lt;matplotlib.collections.PathCollection at 0x11e65afd0&gt; . weight_seq = np.array([46.95, 43.72, 64.78, 32.59, 54.63]) sim_height = np.apply_along_axis( lambda weight: np.random.normal(post[:, 0] + post[:, 1] * (weight - xbar), post[:, 2]), axis=1, arr=weight_seq[:, np.newaxis]) . height_PI = np.quantile(sim_height, [0.055, 0.945], axis=1) . height_PI . array([[148.39192365, 145.50397526, 164.59922844, 134.62916833, 155.53171926], [164.47614296, 160.96243128, 180.80917257, 150.78640977, 171.70537767]]) .",
            "url": "https://nilichen.github.io/dataING/statistical_rethinking/2020/03/03/chapter4.html",
            "relUrl": "/statistical_rethinking/2020/03/03/chapter4.html",
            "date": " • Mar 3, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "Statistical Rethinking Chapter 3 Practice . toc: true | badges: true | comments: true | categories: [statistical_rethinking] | . import random import numpy as np import scipy.stats as stats import matplotlib.pyplot as plt import pymc3 . from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = &quot;all&quot; . def binormial_posterior(p_grid, prior, n, N): assert(len(p_grid) == len(prior)) likelihood = stats.binom.pmf(n, N, p_grid) posterior = likelihood * prior posterior = posterior / sum(posterior) return posterior def sample_posterior(p_grid, posterior, sample_size): assert(len(p_grid) == len(posterior)) samples = np.random.choice(p_grid, size=sample_size, p=posterior, replace=True) return samples . Easy . random.seed(100) p_grid = np.linspace(0, 1, 1000) prior = np.ones(1000) posterior = binormial_posterior(p_grid, prior, 6, 9) samples = sample_posterior(p_grid, posterior, 10000) (samples &lt; 0.2).mean() (samples &gt; 0.8).mean() ((samples &gt; 0.2) &amp; (samples &lt; 0.8)).mean() np.quantile(samples, 0.2) np.quantile(samples, 0.8) pymc3.stats.hpd(samples, 0.66) np.quantile(samples, [0.17, 0.83]) . 0.0008 . 0.1179 . 0.8813 . 0.5125125125125125 . 0.7587587587587588 . array([0.52352352, 0.7977978 ]) . array([0.49349349, 0.77277277]) . Medium . random.seed(100) prior = np.ones(1000) posterior = binormial_posterior(p_grid, prior, 8, 15) samples = sample_posterior(p_grid, posterior, 10000) pymc3.stats.hpd(samples, 0.9) (np.random.binomial(15, samples) == 8).mean() (np.random.binomial(9, samples) == 6).mean() . array([0.33233233, 0.72172172]) . 0.1475 . 0.1815 . prior = np.hstack([np.zeros(500), 2*np.ones(500)]) random.seed(100) posterior = binormial_posterior(p_grid, prior, 8, 15) samples = sample_posterior(p_grid, posterior, 10000) pymc3.stats.hpd(samples, 0.9) print((np.random.binomial(15, samples) == 8).mean(), stats.binom.pmf(8, 15, 0.7)) print((np.random.binomial(9, samples) == 6).mean(), stats.binom.pmf(6, 9, 0.7)) . array([0.5005005 , 0.71171171]) . 0.1544 0.08113003332934526 0.2351 0.2668279319999999 . Hard . birth1 = np.array([1,0,0,0,1,1,0,1,0,1,0,0,1,1,0,1,1,0,0,0,1,0,0,0,1,0, 0,0,0,1,1,1,0,1,0,1,1,1,0,1,0,1,1,0,1,0,0,1,1,0,1,0,0,0,0,0,0,0, 1,1,0,1,0,0,1,0,0,0,1,0,0,1,1,1,1,0,1,0,1,1,1,1,1,0,0,1,0,1,1,0, 1,0,1,1,1,0,1,1,1,1]) birth2 = np.array([0,1,0,1,0,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,0,0,1,1,1,0, 1,1,1,0,1,1,1,0,1,0,0,1,1,1,1,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1, 1,1,1,0,1,1,0,1,1,0,1,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,1,0,0,1,1, 0,0,0,1,1,1,0,0,0,0]) . n = birth1.sum() + birth2.sum() n . 111 . prior = np.ones(1000) random.seed(100) posterior = binormial_posterior(p_grid, prior, n, 200) p_grid[posterior==posterior.max()] . array([0.55455455]) . samples = sample_posterior(p_grid, posterior, 10000) [pymc3.stats.hpd(samples, iv) for iv in [0.5, 0.89, 0.97]] . [array([0.53153153, 0.57757758]), array([0.5015015 , 0.61161161]), array([0.47747748, 0.62662663])] . samples_n = np.random.binomial(200, samples) plt.hist(samples_n) plt.axvline(x=n, c=&#39;r&#39;); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; samples_n = np.random.binomial(100, samples) plt.hist(samples_n) plt.axvline(x=birth1.sum(), c=&#39;r&#39;); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; samples_n = np.random.binomial((birth1==0).sum(), samples) plt.hist(samples_n) plt.axvline(x=birth2[birth1==0].sum(), c=&#39;r&#39;); # NOT INDEPENDENT . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt;",
            "url": "https://nilichen.github.io/dataING/2020/02/20/chapter3.html",
            "relUrl": "/2020/02/20/chapter3.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Example Markdown Post",
            "content": "Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . You can include alert boxes …and… . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://nilichen.github.io/dataING/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://nilichen.github.io/dataING/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  
  

  

  
  

  

  
      ,"page8": {
          "title": "",
          "content": "Warning . Do not manually save images into this folder. This is used by GitHub Actions to automatically copy images. Any images you save into this folder could be deleted at build time. .",
          "url": "https://nilichen.github.io/dataING/images/copied_from_nb/",
          "relUrl": "/images/copied_from_nb/",
          "date": ""
      }
      
  

  
  

}