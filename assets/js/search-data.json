{
  
    
        "post0": {
            "title": "Statistical Rethinking Chapter 5",
            "content": "import numpy as np import pandas as pd import pymc3 as pm import matplotlib.pyplot as plt import seaborn as sns . from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = &quot;all&quot; . from sklearn.preprocessing import StandardScaler d = pd.read_csv( &#39;https://raw.githubusercontent.com/rmcelreath/rethinking/Experimental/data/WaffleDivorce.csv&#39;, sep=&#39;;&#39;) scaler = StandardScaler() d[[&#39;A&#39;, &#39;M&#39;, &#39;D&#39;]] = pd.DataFrame(scaler.fit_transform(d[[&#39;MedianAgeMarriage&#39;, &#39;Marriage&#39;, &#39;Divorce&#39;]])) . sns.pairplot(d[[&#39;A&#39;, &#39;M&#39;, &#39;D&#39;]]) . &lt;seaborn.axisgrid.PairGrid at 0x127c8d390&gt; . 5.1 Spurious association . Marriage rate . N = 1000 with pm.Model() as m5_1: # Since the outcome and the predictor are both standardized, the intercept should end up very close to zero a = pm.Normal(&#39;a&#39;, 0, 0.2) bM = pm.Normal(&#39;bM&#39;, 0, 0.5) sigma = pm.Exponential(&#39;sigma&#39;, 1) mu = a + bM * d.M D = pm.Normal(&#39;D&#39;, mu=mu, sigma=sigma, observed=d.D) mean_q = pm.find_MAP() cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, bM, sigma])) . logp = -79.884, ||grad|| = 66.043: 100%|██████████| 10/10 [00:00&lt;00:00, 1705.35it/s] . Prior | . # extract.prior with m5_1: prior = pm.sample_prior_predictive(N) . x_seq = np.arange(-2, 3) a, bM = prior[&#39;a&#39;], prior[&#39;bM&#39;] for i in range(50): y = a[i] + bM[i] * x_seq plt.plot(x_seq, y, c=&#39;k&#39;, alpha=0.2) plt.show(); . Posterior | . x_seq = np.linspace(-3, 3.2, 30) # link post = np.random.multivariate_normal( [mean_q[k] for k in [&#39;a&#39;, &#39;bM&#39;, &#39;sigma&#39;]], cov_q, size=N) mu = np.apply_along_axis( lambda x: post[:, 0] + post[:, 1] * x, axis=1, arr=x_seq[:, np.newaxis]) mu_mean = mu.mean(axis=1) mu_PI = np.quantile(mu, [0.055, 0.945], axis=1) . plt.scatter(d.M, d.D, alpha=0.2) plt.plot(x_seq, mu_mean, c=&#39;k&#39;) plt.fill_between( x_seq, mu_PI[0], mu_PI[1], color=&#39;grey&#39;) plt.xlabel(&#39;Marriage rate&#39;) plt.ylabel(&#39;Divorce rate&#39;) . &lt;matplotlib.collections.PathCollection at 0x12976e210&gt; . [&lt;matplotlib.lines.Line2D at 0x1242f52d0&gt;] . &lt;matplotlib.collections.PolyCollection at 0x12831ef50&gt; . Text(0.5, 0, &#39;Marriage rate&#39;) . Text(0, 0.5, &#39;Divorce rate&#39;) . Median age marriage . # exercise 1 . Multiple regression . A -&gt; M A -&gt; D . with pm.Model() as m5_3: # Since the outcome and the predictor are both standardized, the intercept should end up very close to zero a = pm.Normal(&#39;a&#39;, 0, 0.2) bA = pm.Normal(&#39;bA&#39;, 0, 0.5) bM = pm.Normal(&#39;bM&#39;, 0, 0.5) sigma = pm.Exponential(&#39;sigma&#39;, 1) mu = a + bA * d.A + bM * d.M D = pm.Normal(&#39;D&#39;, mu=mu, sigma=sigma, observed=d.D) with m5_3: mean_q = pm.find_MAP() cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, bA, bM, sigma])) . logp = -61.064, ||grad|| = 0.004354: 100%|██████████| 13/13 [00:00&lt;00:00, 1431.09it/s] . Important: Once we know median age at marriage for a State, there is little or no additional predictive power in also knowing the rate of marriage in that State. . print(f&quot;mean: {np.array([mean_q[k] for k in [&#39;a&#39;, &#39;bA&#39;, &#39;bM&#39;, &#39;sigma&#39;]]).round(3)}&quot;) print(f&#39;std: {np.sqrt(np.diagonal(cov_q)).round(3)}&#39;) . mean: [ 0. -0.614 -0.065 0.793] std: [0.098 0.151 0.151 0.079] . Counterfactual plots =&gt; Partial Dependence Plot . M changes across the range of values in M_seq, while the other predictor is held constant at its mean—which is zero, because A is standardized | . x_seq = np.linspace(-3, 3.2, 30) o_seq = np.zeros(30) # | A=0 | M | X = np.column_stack([o_seq, x_seq]) # link post = np.random.multivariate_normal( [mean_q[k] for k in [&#39;a&#39;, &#39;bA&#39;, &#39;bM&#39;, &#39;sigma&#39;]], cov_q, size=N) mu = np.apply_along_axis( lambda x: post[:, 0] + post[:, 1] * x[0] + post[:, 2] * x[1], axis=1, arr=X) mu_mean = mu.mean(axis=1) mu_PI = np.quantile(mu, [0.055, 0.945], axis=1) . # sim sim_D = np.apply_along_axis( lambda x: np.random.normal(post[:, 0] + post[:, 1] * x[0] + post[:, 2] * x[1], post[:, 3]), axis=1, arr=X) D_PI = np.quantile(sim_D, [0.055, 0.945], axis=1) . plt.plot(x_seq, mu_mean, c=&#39;k&#39;) plt.fill_between( x_seq, mu_PI[0], mu_PI[1], color=&#39;grey&#39;, alpha=0.6) plt.fill_between( x_seq, D_PI[0], D_PI[1], color=&#39;grey&#39;, alpha=0.3); . Warning: In the small world of the model, it is possible to change median age of marriage without also changing the marriage rate. But is this also possible in the large world of reality? Probably not...In that case, while these counterfactual plots always help in understanding the model, they may also mislead by displaying predictions for impossible combinations of predictor values. . M set to its average and A allowed to vary | . # exercise 3 # hint: only need to change one line of code! . Predictor residual plots . the linear relationship between divorce and marriage rates, having statistically “controlled” for median age of marriage. | . with pm.Model() as m5_4: # Since the outcome and the predictor are both standardized, the intercept should end up very close to zero a = pm.Normal(&#39;a&#39;, 0, 0.2) bAM = pm.Normal(&#39;bAM&#39;, 0, 0.5) sigma = pm.Exponential(&#39;sigma&#39;, 1) mu = a + bAM * d.A M = pm.Normal(&#39;M&#39;, mu=mu, sigma=sigma, observed=d.M) mean_q = pm.find_MAP() cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, bAM, sigma])) . logp = -53.827, ||grad|| = 1.2129e-06: 100%|██████████| 12/12 [00:00&lt;00:00, 454.08it/s] . post = np.random.multivariate_normal( [mean_q[k] for k in [&#39;a&#39;, &#39;bAM&#39;, &#39;sigma&#39;]], cov_q, size=N) mu = np.apply_along_axis( lambda x: post[:, 0] + post[:, 1] * x, axis=1, arr=d.A[:, np.newaxis]) mu_mean = mu.mean(axis=1) mu_resid = d.M - mu_mean . fig, axes = plt.subplots(1, 2, figsize=(8, 4)) axes[0].scatter(d.A, d.M) axes[0].plot(d.A, mu_mean, c=&#39;k&#39;) axes[0].set_xlabel(&#39;Median age marriage&#39;) axes[0].set_ylabel(&#39;Marriage rate&#39;) # seaborn trick to fit linear regression sns.regplot(x=mu_resid, y=d.D, ax=axes[1]); axes[1].set_xlabel(&#39;Marriage rate residual&#39;) axes[1].set_ylabel(&#39;Divorce rate&#39;); . The linear relationship between divorce and median age marriage, having statistically “controlled” for marriage rate | . # exercise 2 . Posterior prediction plots. . Note: What is unusual about Idaho and Utah? Both of these States have large proportions of members of the Church of Jesus Christ of Latter-day Saints, known sometimes as Mormons. Members of this church have low rates of divorce, wherever they live. This suggests that having a finer view on the demographic composition of each State, beyond just median age at marriage, would help a lot to refine our understanding. . 5.2 Masked relationship . Important: A second reason to use more than one predictor variable is to measure the direct influences of multiple factors on an outcome, when none of those influences is apparent from bivariate relationships. This kind of problem tends to arise when there are two predictor variables that are correlated with one another. However, one of these is positively correlated with the outcome and the other is negatively correlated with it. . d = pd.read_csv(&#39;https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/milk.csv&#39;, sep=&#39;;&#39;) d[&#39;lmass&#39;] = np.log(d[&#39;mass&#39;]) scaler = StandardScaler() d[[&#39;K&#39;, &#39;N&#39;, &#39;M&#39;]] = pd.DataFrame(scaler.fit_transform(d[[&#39;kcal.per.g&#39;, &#39;neocortex.perc&#39;, &#39;lmass&#39;]])) sns.pairplot(d[[&#39;K&#39;, &#39;N&#39;, &#39;M&#39;]]) . &lt;seaborn.axisgrid.PairGrid at 0x1204d0150&gt; . M -&gt; K &lt;- N M -&gt; N . Large animals tend to live a long time. And in such animals, an investment in learning may be a better investment, because learning can be amortized over a longer lifespan. Both large body size and large neocortex then influence milk composition, but in different directions, for different reasons...Body mass (M) influences neocortex percent (N). Both then influence kilocalories in milk (K)...But with the evidence at hand, we cannot easily see which is right. . Difference when simulating . Spurious association | . n = 100 x_real = np.random.normal(size=n) x_spur = np.random.normal(x_real, size=n) y = np.random.normal(x_real, size=n) . Masked relationship | . M = np.random.normal(size=n) N = np.random.normal(M, size=n) K = np.random.normal(N - M, size=n) . 5.3 Categorical variables . Binary categories . Indicator variable $$h_i sim Normal( mu_i, sigma)$$ $$ mu_i sim alpha + beta_i m_i$$ $$ alpha sim Normal(178, 20)$$ $$ beta_m sim Normal(0, 10)$$ $$ sigma sim Uniform(0, 50)$$ Warning: This can make assigning sensible priors a little harder...Furthermore, this approach necessarily assumes there is more uncertainty about one of the categories - Index variable $$h_i sim Normal( mu_i, sigma)$$ $$ mu_i sim alpha_{SEX[i]}$$ $$ alpha_j sim Normal(178, 20), for j=1..2$$ $$ sigma sim Uniform(0, 50)$$ | . d = pd.read_csv(&#39;https://raw.githubusercontent.com/rmcelreath/rethinking/Experimental/data/Howell1.csv&#39;, sep=&#39;;&#39;) . with pm.Model() as m5_8: a = pm.Normal(&#39;a&#39;, 178, 20, shape=d[&#39;male&#39;].nunique()) sigma = pm.Uniform(&#39;sigma&#39;, 0, 50) mu = a[d[&#39;male&#39;].values] height = pm.Normal(&#39;height&#39;, mu, sigma, observed=d[&#39;height&#39;]) mean_q = pm.find_MAP() cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, sigma])) . logp = -2,724.8, ||grad|| = 0.026744: 100%|██████████| 14/14 [00:00&lt;00:00, 1277.03it/s] . mean_q . {&#39;a&#39;: array([135.55461973, 143.16814912]), &#39;sigma_interval__&#39;: array(18.28842903), &#39;sigma&#39;: array(49.99999943)} . cov_q . array([[ 8.46804887, -0.05219947, -1.57797704], [ -0.05219947, 9.44895653, -1.44254616], [ -1.57797704, -1.44254616, -43.60780918]]) . Many categories . d = pd.read_csv(&#39;https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/milk.csv&#39;, sep=&#39;;&#39;) d[&#39;K&#39;] = scaler.fit_transform(d[[&#39;kcal.per.g&#39;]]) . with pm.Model() as m5_9: a = pm.Normal(&#39;a&#39;, 0, 0.5, shape=d[&#39;clade&#39;].nunique()) sigma = pm.Exponential(&#39;sigma&#39;, 1) mu = a[d[&#39;clade&#39;].astype(&#39;category&#39;).cat.codes.values] K = pm.Normal(&#39;K&#39;, mu, sigma, observed=d[&#39;K&#39;]) mean_q = pm.find_MAP() cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, sigma])) . logp = -36.531, ||grad|| = 0.0019242: 100%|██████████| 13/13 [00:00&lt;00:00, 2109.32it/s] . mean_q . {&#39;a&#39;: array([-0.48941463, 0.37008325, 0.68048305, -0.58956155]), &#39;sigma_log__&#39;: array(-0.31019941), &#39;sigma&#39;: array(0.73330072)} . np.sqrt(np.diagonal(cov_q)) . array([0.22105719, 0.2204332 , 0.26134171, 0.27832982, 0.0985117 ]) . Pracitice . Hard . foxes = pd.read_csv(&#39;https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/foxes.csv&#39;, sep=&#39;;&#39;) foxes[[&#39;F&#39;, &#39;G&#39;, &#39;A&#39;, &#39;W&#39;]] = pd.DataFrame(scaler.fit_transform(foxes[[&#39;avgfood&#39;, &#39;groupsize&#39;, &#39;area&#39;, &#39;weight&#39;]])) . sns.pairplot(foxes[[&#39;F&#39;, &#39;G&#39;, &#39;A&#39;, &#39;W&#39;]]) . &lt;seaborn.axisgrid.PairGrid at 0x1256ea250&gt; . def get_post(X, y, N=1000): with pm.Model() as model: # Since the outcome and the predictor are both standardized, the intercept should end up very close to zero a = pm.Normal(&#39;a&#39;, 0, 0.2) b = pm.Normal(&#39;b&#39;, 0, 0.5, shape=X.shape[1]) sigma = pm.Exponential(&#39;sigma&#39;, 1) y = pm.Normal(&#39;y&#39;, mu=a + pm.math.dot(X, b), sigma=sigma, observed=y) mean_q = pm.find_MAP() means = np.concatenate([mean_q[k].reshape(-1) for k in [&#39;a&#39;, &#39;b&#39;, &#39;sigma&#39;]]) cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, b, sigma])) stds = np.sqrt(np.diagonal(cov_q)) post = np.random.multivariate_normal(means, cov_q, size=N) print(&#39;means: &#39;, means.round(3)) print(&#39;stds: &#39;, stds.round(3)) return post . def plot_intervals(post, X): X = np.concatenate((np.ones(30)[:, np.newaxis], X), axis=1) # link mu = X.dot(post[:, :-1].T) mu_mean = mu.mean(axis=1) mu_PI = np.quantile(mu, [0.055, 0.945], axis=1) # sim sim_D = np.random.normal(mu, post[:, -1]) D_PI = np.quantile(sim_D, [0.055, 0.945], axis=1) plt.plot(x_seq, mu_mean, c=&#39;k&#39;) plt.fill_between( x_seq, mu_PI[0], mu_PI[1], color=&#39;grey&#39;, alpha=0.6) plt.fill_between( x_seq, D_PI[0], D_PI[1], color=&#39;grey&#39;, alpha=0.3); . 5H1 (1) | . post = get_post(foxes[[&#39;A&#39;]], foxes[&#39;W&#39;]) . logp = -185.03, ||grad|| = 124.83: 100%|██████████| 9/9 [00:00&lt;00:00, 649.03it/s] . means: [0. 0.019 0.996] stds: [0.084 0.091 0.065] . x_seq = np.linspace(-3, 3.2, 30) plot_intervals(post, x_seq[:, np.newaxis]) . 5H1 (2) | . post = get_post(foxes[[&#39;G&#39;]], foxes[&#39;W&#39;]) x_seq = np.linspace(-3, 3.2, 30) . logp = -185.03, ||grad|| = 130.66: 100%|██████████| 9/9 [00:00&lt;00:00, 1337.28it/s] . means: [ 0. -0.156 0.983] stds: [0.083 0.09 0.064] . plot_intervals(post, x_seq[:, np.newaxis]) . 5H2 | . post = get_post(foxes[[&#39;A&#39;, &#39;G&#39;]], foxes[&#39;W&#39;]) x_seq = np.linspace(-3, 3.2, 30) . logp = -185.26, ||grad|| = 130.75: 100%|██████████| 11/11 [00:00&lt;00:00, 1709.93it/s] . means: [ 0. 0.406 -0.482 0.946] stds: [0.08 0.145 0.145 0.062] . # holding groupsize constant at 0 plot_intervals(post, np.column_stack([np.linspace(-3, 3.2, 30), np.zeros(30)])) . # holding area constant at 0 plot_intervals(post, np.column_stack([np.zeros(30), np.linspace(-3, 3.2, 30)])) . 5H3 | . post = get_post(foxes[[&#39;F&#39;, &#39;G&#39;]], foxes[&#39;W&#39;]) x_seq = np.linspace(-3, 3.2, 30) . logp = -160.46, ||grad|| = 5.1972e-06: 100%|██████████| 12/12 [00:00&lt;00:00, 1871.48it/s] . means: [ 0. 0.477 -0.574 0.946] stds: [0.08 0.179 0.179 0.062] . # holding groupsize constant at 0 plot_intervals(post, np.column_stack([np.linspace(-3, 3.2, 30), np.zeros(30)])) . # holding avgfood constant at 0 plot_intervals(post, np.column_stack([np.zeros(30), np.linspace(-3, 3.2, 30)])) . post = get_post(foxes[[&#39;A&#39;, &#39;F&#39;, &#39;G&#39;]], foxes[&#39;W&#39;]) . logp = -159.37, ||grad|| = 1.2402: 100%|██████████| 16/16 [00:00&lt;00:00, 1092.30it/s] . means: [ 0. 0.278 0.297 -0.64 0.935] stds: [0.08 0.17 0.21 0.182 0.061] .",
            "url": "https://nilichen.github.io/dataING/statistical_rethinking/2020/03/05/chapter5.html",
            "relUrl": "/statistical_rethinking/2020/03/05/chapter5.html",
            "date": " • Mar 5, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Statistical Rethinking Chapter 4",
            "content": "import numpy as np import scipy.stats as stats import pandas as pd import matplotlib.pyplot as plt import pymc3 as pm . df = pd.read_csv(&#39;https://raw.githubusercontent.com/rmcelreath/rethinking/Experimental/data/Howell1.csv&#39;, sep=&#39;;&#39;) . df2 = df[df.age &gt;= 18] . 4.3 Gaussian model of height . Prior predictive simulation . effect of $ mu$ prior | . sample_mu = np.random.normal(178, 20, 10000) sample_sigma = np.random.uniform(0, 50, 10000) prior_h = np.random.normal(sample_mu, sample_sigma) fig, axes = plt.subplots(2, sharex=True) axes[0].hist(prior_h); sample_mu = np.random.normal(178, 100, 10000) prior_h = np.random.normal(sample_mu, sample_sigma) axes[1].hist(prior_h); . Grid Approximation . mu_list = np.linspace(150, 160, 100) sigma_list = np.linspace(7, 9, 100) post_mu, post_sigma = np.array(np.meshgrid(mu_list, sigma_list))#.reshape(2, 10000).T # post.shape . # log likelihood post_LL = np.vectorize( lambda mu, sigma: stats.norm(mu, sigma).logpdf(df2.height).sum() )(post_mu, post_sigma) post_LL.shape . (100, 100) . # unnormalized posterior post_prod = (post_LL + stats.norm(178, 20).logpdf(post_mu) + stats.uniform(0, 50).logpdf(post_sigma)) post_prob = np.exp(post_prod - np.max(post_prod)) # normalized post_prob = post_prob / post_prob.sum() post_prob.shape . (100, 100) . fig, axes = plt.subplots(1, 2, figsize=(10, 4)) axes[0].pcolormesh(post_mu, post_sigma, post_prob, cmap=&#39;Greys&#39;) axes[1].contour(post_mu, post_sigma, post_prob, cmap=&#39;Greys&#39;) # plt.colorbar() . &lt;matplotlib.contour.QuadContourSet at 0x1196f7490&gt; . Sample from the posterior . compare with prior distribution $$ mu sim Normal(178, 20)$$ $$ sigma sim Uniform(0, 50)$$ | . sample_mu = np.random.choice(post_mu.reshape(10000,), size=10000, p=post_prob.reshape(10000,), replace=True) sample_sigma = np.random.choice(post_sigma.reshape(10000,), size=10000, p=post_prob.reshape(10000,), replace=True) . fig, axes = plt.subplots(1, 3, figsize=(12, 4)) axes[0].scatter(sample_mu, sample_sigma, alpha=0.01) # axes[0].xlim(150, 160) axes[1].hist(sample_mu) # axes[1].hist(np.random.normal(178, 20, 10000), alpha=0.1) axes[2].hist(sample_sigma); . sample_mu.shape . (10000,) . Sample size . df3 = df2.sample(20) mu_list = np.linspace(150, 170, 100) sigma_list = np.linspace(4, 20, 100) post_mu, post_sigma = np.array(np.meshgrid(mu_list, sigma_list))#.reshape(2, 10000).T . # log likelihood post_LL = np.vectorize( lambda mu, sigma: stats.norm(mu, sigma).logpdf(df3.height).sum() )(post_mu, post_sigma) post_LL.shape . (100, 100) . # unnormalized posterior post_prod = (post_LL + stats.norm(178, 20).logpdf(post_mu) + stats.uniform(0, 50).logpdf(post_sigma)) post_prob = np.exp(post_prod - np.max(post_prod)) # normalized post_prob = post_prob / post_prob.sum() post_prob.shape . (100, 100) . sample_mu = np.random.choice(post_mu.reshape(10000,), size=10000, p=post_prob.reshape(10000,), replace=True) sample_sigma = np.random.choice(post_sigma.reshape(10000,), size=10000, p=post_prob.reshape(10000,), replace=True) . In principle, the posterior is not always so Gaussian in shape...you do need to be careful of abusing the quadratic approximation...But a useful way to conceive of the problem is that variances must be positive. As a result, there must be more uncertainty about how big the variance (or standard deviation) is than about how small it is. . fig, axes = plt.subplots(1, 3, figsize=(12, 4)) axes[0].scatter(sample_mu, sample_sigma, alpha=0.02) # axes[0].xlim(150, 160) axes[1].hist(sample_mu) # axes[1].hist(np.random.normal(178, 20, 10000), alpha=0.1) axes[2].hist(sample_sigma); . Quadratic approximation . with pm.Model() as normal_approximation: mu = pm.Normal(&#39;mu&#39;, 178, 20) sigma = pm.Uniform(&#39;sigma&#39;, 0, 50) height = pm.Normal(&#39;h&#39;, mu=mu, sigma=sigma, observed=df2.height) # https://docs.pymc.io/notebooks/getting_started.html#Maximum-a-posteriori-methods mean_q = pm.find_MAP() # https://stats.stackexchange.com/questions/68080/basic-question-about-fisher-information-matrix-and-relationship-to-hessian-and-s cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[mu, sigma])) . logp = -1,235.2, ||grad|| = 11.697: 100%|██████████| 19/19 [00:00&lt;00:00, 1548.98it/s] . mean_q . {&#39;mu&#39;: array(154.60702358), &#39;sigma_interval__&#39;: array(-1.69876478), &#39;sigma&#39;: array(7.73133303)} . cov_q . array([[0.16973961, 0.00021803], [0.00021803, 0.08490583]]) . post = np.random.multivariate_normal([mean_q[&#39;mu&#39;], mean_q[&#39;sigma&#39;]], cov_q, size=1000) . fig, axes = plt.subplots(1, 2, figsize=(8, 4)) axes[0].hist(post[:, 0]) axes[1].hist(post[:, 1]); . 4.4 Linear Prediction . Prior . np.random.seed(888) N = 100 a = np.random.normal(178, 20, N) b = np.random.normal(0, 10, N) lb = np.random.lognormal(0, 1, N) fig, axes = plt.subplots(1, 2, figsize=(8, 4), sharey=True) axes[0].hist(b) axes[1].hist(lb); . x = np.array([df2.weight.min(), df2.weight.max()]) xbar = df2.weight.mean() fig, axes = plt.subplots(1, 2, figsize=(8, 4), sharey=True) for i in range(N): y = a[i] + b[i] * (x - xbar) ly = a[i] + lb[i] * (x - xbar) axes[0].plot(x, y, c=&#39;k&#39;, alpha=0.2) axes[1].plot(x, ly, c=&#39;k&#39;, alpha=0.2) . Posterior Distribution . def fit_linear(x, y): with pm.Model() as normal_approximation: a = pm.Normal(&#39;a&#39;, 178, 20) b = pm.Lognormal(&#39;b&#39;, 0, 1) sigma = pm.Uniform(&#39;sigma&#39;, 0, 50) mu = a + b * (x - x.mean()) height = pm.Normal(&#39;h&#39;, mu=mu, sigma=sigma, observed=y) mean_q = pm.find_MAP() cov_q = np.linalg.inv(pm.find_hessian(mean_q, vars=[a, b, sigma])) return mean_q, cov_q . mean_q, cov_q = fit_linear(df2.weight, df2.height) . logp = -1,080.4, ||grad|| = 5.147: 100%|██████████| 39/39 [00:00&lt;00:00, 2308.66it/s] . mean_q . {&#39;a&#39;: array(154.60136748), &#39;b_log__&#39;: array(-0.10172172), &#39;sigma_interval__&#39;: array(-2.18135226), &#39;b&#39;: array(0.90328088), &#39;sigma&#39;: array(5.07188029)} . cov_q.round(3) . array([[ 0.073, -0. , 0. ], [-0. , 0.002, -0. ], [ 0. , -0. , 0.037]]) . post = np.random.multivariate_normal( [mean_q[&#39;a&#39;], mean_q[&#39;b&#39;], mean_q[&#39;sigma&#39;]], cov_q, size=1000) a_map = post[:, 0].mean() b_map = post[:, 1].mean() plt.scatter(df2.weight, df2.height) plt.plot(x, a_map + b_map * (x - df2.weight.mean()), c=&#39;k&#39;, linewidth=2) . [&lt;matplotlib.lines.Line2D at 0x11d0d54d0&gt;] . Uncertainty . def plot_lines(df, ax, n=20): mean_q, cov_q = fit_linear(df.weight, df.height) post = np.random.multivariate_normal( [mean_q[&#39;a&#39;], mean_q[&#39;b&#39;], mean_q[&#39;sigma&#39;]], cov_q, size=n) ax.scatter(df.weight, df.height) for i in range(n): y = post[i, 0] + post[i, 1] * (x - xbar) ax.plot(x, y, c=&#39;k&#39;, alpha=0.2) . fig, axes = plt.subplots(1, 2, figsize=(8, 4), sharey=True) plot_lines(df2[:10], axes[0]) plot_lines(df2, axes[1]) . logp = -38.38, ||grad|| = 3.086e-05: 100%|██████████| 32/32 [00:00&lt;00:00, 2187.88it/s] logp = -1,080.4, ||grad|| = 5.147: 100%|██████████| 39/39 [00:00&lt;00:00, 2800.13it/s] . Regression Intervals . weight_seq = np.arange(25, 71) # link mu = np.apply_along_axis( lambda weight: post[:, 0] + post[:, 1] * (weight - xbar), axis=1, arr=weight_seq[:, np.newaxis]) . mu_mean = mu.mean(axis=1) mu_PI = np.quantile(mu, [0.055, 0.945], axis=1) . # mu_PI . fig, axes = plt.subplots(1, 2, figsize=(8, 4)) for i in range(100): axes[0].scatter(weight_seq, mu[:, i], c=&#39;b&#39;, alpha=0.02, s=5) axes[1].scatter(df2.weight, df2.height, alpha=0.2) axes[1].plot(weight_seq, mu_mean, c=&#39;k&#39;) axes[1].fill_between( weight_seq, mu_PI[0], mu_PI[1], color=&#39;grey&#39;, alpha=0.6) . &lt;matplotlib.collections.PolyCollection at 0x11f0b92d0&gt; . Prediction Intervals . Rethinking: Two kinds of uncertainty . # sim sim_height = np.apply_along_axis( lambda weight: np.random.normal(post[:, 0] + post[:, 1] * (weight - xbar), post[:, 2]), axis=1, arr=weight_seq[:, np.newaxis]) # sim_height = np.random.normal(mu, post[:, 2]) . height_PI = np.quantile(sim_height, [0.055, 0.945], axis=1) . plt.scatter(df2.weight, df2.height, alpha=0.2) plt.plot(weight_seq, mu_mean, c=&#39;k&#39;) plt.fill_between( weight_seq, mu_PI[0], mu_PI[1], color=&#39;grey&#39;, alpha=0.6) plt.fill_between( weight_seq, height_PI[0], height_PI[1], color=&#39;grey&#39;, alpha=0.3) . &lt;matplotlib.collections.PolyCollection at 0x11fb71950&gt; . Practice . plt.scatter(df.weight, df.height, c=df.age &gt;= 18, cmap=&#39;tab10&#39;) . &lt;matplotlib.collections.PathCollection at 0x11e65afd0&gt; . weight_seq = np.array([46.95, 43.72, 64.78, 32.59, 54.63]) sim_height = np.apply_along_axis( lambda weight: np.random.normal(post[:, 0] + post[:, 1] * (weight - xbar), post[:, 2]), axis=1, arr=weight_seq[:, np.newaxis]) . height_PI = np.quantile(sim_height, [0.055, 0.945], axis=1) . height_PI . array([[148.39192365, 145.50397526, 164.59922844, 134.62916833, 155.53171926], [164.47614296, 160.96243128, 180.80917257, 150.78640977, 171.70537767]]) .",
            "url": "https://nilichen.github.io/dataING/statistical_rethinking/2020/03/03/chapter4.html",
            "relUrl": "/statistical_rethinking/2020/03/03/chapter4.html",
            "date": " • Mar 3, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Statistical Rethinking Chapter 3 Practice",
            "content": "import random import numpy as np import scipy.stats as stats import matplotlib.pyplot as plt import pymc3 . from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = &quot;all&quot; . def binormial_posterior(p_grid, prior, n, N): assert(len(p_grid) == len(prior)) likelihood = stats.binom.pmf(n, N, p_grid) posterior = likelihood * prior posterior = posterior / sum(posterior) return posterior def sample_posterior(p_grid, posterior, sample_size): assert(len(p_grid) == len(posterior)) samples = np.random.choice(p_grid, size=sample_size, p=posterior, replace=True) return samples . Easy . random.seed(100) p_grid = np.linspace(0, 1, 1000) prior = np.ones(1000) posterior = binormial_posterior(p_grid, prior, 6, 9) samples = sample_posterior(p_grid, posterior, 10000) (samples &lt; 0.2).mean() (samples &gt; 0.8).mean() ((samples &gt; 0.2) &amp; (samples &lt; 0.8)).mean() np.quantile(samples, 0.2) np.quantile(samples, 0.8) pymc3.stats.hpd(samples, 0.66) np.quantile(samples, [0.17, 0.83]) . 0.0008 . 0.1179 . 0.8813 . 0.5125125125125125 . 0.7587587587587588 . array([0.52352352, 0.7977978 ]) . array([0.49349349, 0.77277277]) . Medium . random.seed(100) prior = np.ones(1000) posterior = binormial_posterior(p_grid, prior, 8, 15) samples = sample_posterior(p_grid, posterior, 10000) pymc3.stats.hpd(samples, 0.9) (np.random.binomial(15, samples) == 8).mean() (np.random.binomial(9, samples) == 6).mean() . array([0.33233233, 0.72172172]) . 0.1475 . 0.1815 . prior = np.hstack([np.zeros(500), 2*np.ones(500)]) random.seed(100) posterior = binormial_posterior(p_grid, prior, 8, 15) samples = sample_posterior(p_grid, posterior, 10000) pymc3.stats.hpd(samples, 0.9) print((np.random.binomial(15, samples) == 8).mean(), stats.binom.pmf(8, 15, 0.7)) print((np.random.binomial(9, samples) == 6).mean(), stats.binom.pmf(6, 9, 0.7)) . array([0.5005005 , 0.71171171]) . 0.1544 0.08113003332934526 0.2351 0.2668279319999999 . Hard . birth1 = np.array([1,0,0,0,1,1,0,1,0,1,0,0,1,1,0,1,1,0,0,0,1,0,0,0,1,0, 0,0,0,1,1,1,0,1,0,1,1,1,0,1,0,1,1,0,1,0,0,1,1,0,1,0,0,0,0,0,0,0, 1,1,0,1,0,0,1,0,0,0,1,0,0,1,1,1,1,0,1,0,1,1,1,1,1,0,0,1,0,1,1,0, 1,0,1,1,1,0,1,1,1,1]) birth2 = np.array([0,1,0,1,0,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,0,0,1,1,1,0, 1,1,1,0,1,1,1,0,1,0,0,1,1,1,1,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1, 1,1,1,0,1,1,0,1,1,0,1,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,1,0,0,1,1, 0,0,0,1,1,1,0,0,0,0]) . n = birth1.sum() + birth2.sum() n . 111 . prior = np.ones(1000) random.seed(100) posterior = binormial_posterior(p_grid, prior, n, 200) p_grid[posterior==posterior.max()] . array([0.55455455]) . samples = sample_posterior(p_grid, posterior, 10000) [pymc3.stats.hpd(samples, iv) for iv in [0.5, 0.89, 0.97]] . [array([0.53153153, 0.57757758]), array([0.5015015 , 0.61161161]), array([0.47747748, 0.62662663])] . samples_n = np.random.binomial(200, samples) plt.hist(samples_n) plt.axvline(x=n, c=&#39;r&#39;); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; samples_n = np.random.binomial(100, samples) plt.hist(samples_n) plt.axvline(x=birth1.sum(), c=&#39;r&#39;); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; samples_n = np.random.binomial((birth1==0).sum(), samples) plt.hist(samples_n) plt.axvline(x=birth2[birth1==0].sum(), c=&#39;r&#39;); # NOT INDEPENDENT . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt;",
            "url": "https://nilichen.github.io/dataING/statistical_rethinking/2020/02/20/chapter3.html",
            "relUrl": "/statistical_rethinking/2020/02/20/chapter3.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Example Markdown Post",
            "content": "Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . You can include alert boxes …and… . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://nilichen.github.io/dataING/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://nilichen.github.io/dataING/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  
  

  

  
  

  

  
      ,"page8": {
          "title": "",
          "content": "Warning . Do not manually save images into this folder. This is used by GitHub Actions to automatically copy images. Any images you save into this folder could be deleted at build time. .",
          "url": "https://nilichen.github.io/dataING/images/copied_from_nb/",
          "relUrl": "/images/copied_from_nb/",
          "date": ""
      }
      
  

  
  

}